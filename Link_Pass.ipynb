{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AshKetchum6522/Link-Bypasser/blob/main/Link_Pass.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter AdFly Link Below!**"
      ],
      "metadata": {
        "id": "rWggyWOjc2vK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown <center><img src='https://drfilestreambot.cf/AgADRA14795' height=\"100\">\n",
        "#@title <b><center>Enter AdFly Link Below</center></b>\n",
        "!pip install cloudscraper\n",
        "import cloudscraper\n",
        "import re\n",
        "from base64 import b64decode\n",
        "from urllib.parse import unquote\n",
        "\n",
        "# ==========================================\n",
        "\n",
        "url = \"\" #@param {type:\"string\"}\n",
        "'''\n",
        "404: Complete exception handling not found :(\n",
        "'''\n",
        "# ==========================================\n",
        "\n",
        "def decrypt_url(code):\n",
        "    a, b = '', ''\n",
        "    for i in range(0, len(code)):\n",
        "        if i % 2 == 0: a += code[i]\n",
        "        else: b = code[i] + b\n",
        "    key = list(a + b)\n",
        "    i = 0\n",
        "    while i < len(key):\n",
        "        if key[i].isdigit():\n",
        "            for j in range(i+1,len(key)):\n",
        "                if key[j].isdigit():\n",
        "                    u = int(key[i]) ^ int(key[j])\n",
        "                    if u < 10: key[i] = str(u)\n",
        "                    i = j\n",
        "                    break\n",
        "        i+=1\n",
        "    key = ''.join(key)\n",
        "    decrypted = b64decode(key)[16:-16]\n",
        "    return decrypted.decode('utf-8')\n",
        "\n",
        "# ==========================================\n",
        "\n",
        "def adfly(url):\n",
        "    client = cloudscraper.create_scraper(allow_brotli=False)\n",
        "    res = client.get(url).text\n",
        "    out = {'error': False, 'src_url': url}\n",
        "    try:\n",
        "        ysmm = re.findall(\"ysmm\\s+=\\s+['|\\\"](.*?)['|\\\"]\", res)[0]\n",
        "    except:\n",
        "        out['error'] = True\n",
        "        return out\n",
        "    url = decrypt_url(ysmm)\n",
        "    if re.search(r'go\\.php\\?u\\=', url):\n",
        "        url = b64decode(re.sub(r'(.*?)u=', '', url)).decode()\n",
        "    elif '&dest=' in url:\n",
        "        url = unquote(re.sub(r'(.*?)dest=', '', url))\n",
        "    out['bypassed_url'] = url\n",
        "    return out\n",
        "\n",
        "# ==========================================\n",
        "\n",
        "res = adfly(url)\n",
        "\n",
        "print(res)"
      ],
      "metadata": {
        "id": "NRE5wW9wc85c",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter GPLinks Link Below!**"
      ],
      "metadata": {
        "id": "vIuY_F5gVVm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown <center><img src='https://drfilestreambot.cf/AgADRQ14796' height=\"100\">\n",
        "#@title <b><center>Enter GPLinks Link Below</center></b>\n",
        "print(\"Downloading Cloud-Scraper...\")\n",
        "!pip install cloudscraper\n",
        "print(\"Setting Up!\")\n",
        "print(\"Performing Check...\")\n",
        "import time\n",
        "import cloudscraper\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "print(\"Everything Looks Good! Lets Continue.\")\n",
        "\n",
        "url = \"\" #@param {type:\"string\"}\n",
        "print(\"Entered Link:\")\n",
        "print(url)\n",
        "print(\"Checking Link...\")\n",
        "print(\"Checking Done!\")\n",
        "print(\"Bypassing Link...\")\n",
        "# ==============================================\n",
        "\n",
        "def gplinks(url: str):\n",
        "    client = cloudscraper.create_scraper(allow_brotli=False)\n",
        "    p = urlparse(url)\n",
        "    final_url = f\"{p.scheme}://{p.netloc}/links/go\"\n",
        "    res = client.head(url)\n",
        "    header_loc = res.headers[\"location\"]\n",
        "    p = urlparse(header_loc)\n",
        "    ref_url = f\"{p.scheme}://{p.netloc}/\"\n",
        "    h = {\"referer\": ref_url}\n",
        "    res = client.get(url, headers=h, allow_redirects=False)\n",
        "    bs4 = BeautifulSoup(res.content, \"html.parser\")\n",
        "    inputs = bs4.find_all(\"input\")\n",
        "    time.sleep(10) # !important\n",
        "    data = { input.get(\"name\"): input.get(\"value\") for input in inputs }\n",
        "    h = {\n",
        "        \"content-type\": \"application/x-www-form-urlencoded\",\n",
        "        \"x-requested-with\": \"XMLHttpRequest\"\n",
        "    }\n",
        "    time.sleep(10)\n",
        "    res = client.post(final_url, headers=h, data=data)\n",
        "    try:\n",
        "        return res.json()[\"url\"].replace(\"/\",\"/\")\n",
        "    except:\n",
        "        return \"Could not Bypass your URL :(\"\n",
        "\n",
        "# ==============================================\n",
        "\n",
        "res = gplinks(url)\n",
        "\n",
        "print(res)\n",
        "print(\"Successfully Bypassed!\")"
      ],
      "metadata": {
        "id": "PHCG2iGQgDhz",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter GDTot Link as well as your GDTot Crypt! If you don't know how to get Crypt then <a href=\"https://www.youtube.com/watch?v=EfZ29CotRSU\">Learn Here</a>**"
      ],
      "metadata": {
        "id": "cyBiaGkAUtLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown <center><img src='https://drfilestreambot.cf/AgADQg14793' height=\"100\">\n",
        "#@title <b><center>Enter GDTot-Link Below</center></b>\n",
        "!pip install cloudscraper\n",
        "import cloudscraper\n",
        "import re\n",
        "import base64\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "\n",
        "url = \"\" #@param {type:\"string\"}\n",
        "GDTot_Crypt = \"b0lDek5LSCt6ZjVRR2EwZnY4T1EvVndqeDRtbCtTWmMwcGNuKy8wYWpDaz0%3D\" #@param {type:\"string\"}\n",
        "# ==========================================\n",
        "\n",
        "def gdtot(url):\n",
        "    client = cloudscraper.create_scraper(allow_brotli=False)\n",
        "    match = re.findall(r\"https?://(.+)\\.gdtot\\.(.+)\\/\\S+\\/\\S+\", url)[0]\n",
        "    client.cookies.update({ \"crypt\": GDTot_Crypt })\n",
        "    res = client.get(url)\n",
        "    res = client.get(f\"https://{match[0]}.gdtot.{match[1]}/dld?id={url.split('/')[-1]}\")\n",
        "    url = re.findall(r'URL=(.*?)\"', res.text)[0]\n",
        "    info = {}\n",
        "    info[\"error\"] = False\n",
        "    params = parse_qs(urlparse(url).query)\n",
        "    if \"gd\" not in params or not params[\"gd\"] or params[\"gd\"][0] == \"false\":\n",
        "        info[\"error\"] = True\n",
        "        if \"msgx\" in params:\n",
        "            info[\"message\"] = params[\"msgx\"][0]\n",
        "        else:\n",
        "            info[\"message\"] = \"Invalid link\"\n",
        "    else:\n",
        "        decoded_id = base64.b64decode(str(params[\"gd\"][0])).decode(\"utf-8\")\n",
        "        drive_link = f\"https://drive.google.com/open?id={decoded_id}\"\n",
        "        info[\"gdrive_link\"] = drive_link\n",
        "    if not info[\"error\"]:\n",
        "        return info[\"gdrive_link\"]\n",
        "    else:\n",
        "        return \"Could not generate GDrive URL for your GDTot Link :(\"\n",
        "\n",
        "# ==========================================\n",
        "\n",
        "res = gdtot(url)\n",
        "\n",
        "print(res)"
      ],
      "metadata": {
        "id": "pJCdd8LESBAk",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter Sharer.pw Link, XSRF_TOKEN and laravel_session cookies! If you don't know how to get then then watch this <a href=\"https://www.youtube.com/watch?v=EfZ29CotRSU\">Video</a> (for GDTOT) and do the same for Sharer.pw**"
      ],
      "metadata": {
        "id": "JlOUDYIzlLTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown <center><img src='https://drfilestreambot.cf/AgADRg14797' height=\"50\">\n",
        "#@title <b><center>Enter Sharer.pw Link Below</center></b>\n",
        "!pip install cloudscraper\n",
        "import cloudscraper\n",
        "import re\n",
        "from lxml import etree\n",
        "\n",
        "url = \"\" #@param {type:\"string\"}\n",
        "XSRF_TOKEN = \"\" #@param {type:\"string\"}\n",
        "Laravel_Session = \"\" #@param {type:\"string\"}\n",
        "'''\n",
        "404: Exception Handling Not Found :(\n",
        "NOTE:\n",
        "DO NOT use the logout button on website. Instead, clear the site cookies manually to log out.\n",
        "If you use logout from website, cookies will become invalid.\n",
        "'''\n",
        "\n",
        "# ===================================================================\n",
        "\n",
        "def parse_info(res):\n",
        "    f = re.findall(\">(.*?)<\\/td>\", res.text)\n",
        "    info_parsed = {}\n",
        "    for i in range(0, len(f), 3):\n",
        "        info_parsed[f[i].lower().replace(' ', '_')] = f[i+2]\n",
        "    return info_parsed\n",
        "\n",
        "def sharer_pw(url, forced_login=False):\n",
        "    client = cloudscraper.create_scraper(allow_brotli=False)\n",
        "    client.cookies.update({\n",
        "        \"XSRF-TOKEN\": XSRF_TOKEN,\n",
        "        \"laravel_session\": Laravel_Session\n",
        "    })\n",
        "    res = client.get(url)\n",
        "    token = re.findall(\"_token\\s=\\s'(.*?)'\", res.text, re.DOTALL)[0]\n",
        "    ddl_btn = etree.HTML(res.content).xpath(\"//button[@id='btndirect']\")\n",
        "    info_parsed = parse_info(res)\n",
        "    info_parsed['error'] = True\n",
        "    info_parsed['src_url'] = url\n",
        "    info_parsed['link_type'] = 'login'\n",
        "    info_parsed['forced_login'] = forced_login\n",
        "    headers = {\n",
        "        'content-type': 'application/x-www-form-urlencoded; charset=UTF-8',\n",
        "        'x-requested-with': 'XMLHttpRequest'\n",
        "    }\n",
        "    data = {\n",
        "        '_token': token\n",
        "    }\n",
        "    if len(ddl_btn):\n",
        "        info_parsed['link_type'] = 'direct'\n",
        "    if not forced_login:\n",
        "        data['nl'] = 1\n",
        "    try:\n",
        "        res = client.post(url+'/dl', headers=headers, data=data).json()\n",
        "    except:\n",
        "        return info_parsed\n",
        "    if 'url' in res and res['url']:\n",
        "        info_parsed['error'] = False\n",
        "        info_parsed['gdrive_link'] = res['url']\n",
        "    if len(ddl_btn) and not forced_login and not 'url' in info_parsed:\n",
        "        # retry download via login\n",
        "        return sharer_pw(url, forced_login=True)\n",
        "    return info_parsed\n",
        "\n",
        "# ===================================================================\n",
        "\n",
        "res = sharer_pw(url)\n",
        "\n",
        "print(res)"
      ],
      "metadata": {
        "id": "jY_RrpdYiTqj",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter DropLink Below!**"
      ],
      "metadata": {
        "id": "2PlnCgEllyT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown <center><img src='https://drfilestreambot.cf/AgADRw14798' height=\"75\">\n",
        "#@title <b><center>Enter Drop Link Below</center></b>\n",
        "!pip install cloudscraper\n",
        "import cloudscraper\n",
        "\n",
        "url = \"\" #@param {type:\"string\"}\n",
        "# ==============================================\n",
        "\n",
        "def droplink(url):\n",
        "    api = \"https://api.emilyx.in/api\"\n",
        "    client = cloudscraper.create_scraper(allow_brotli=False)\n",
        "    resp = client.get(url)\n",
        "    if resp.status_code == 404:\n",
        "        return \"File not found/The link you entered is wrong!\"\n",
        "    try:\n",
        "        resp = client.post(api, json={\"type\": \"droplink\", \"url\": url})\n",
        "        res = resp.json()\n",
        "    except BaseException:\n",
        "        return \"API UnResponsive / Invalid Link !\"\n",
        "    if res[\"success\"] is True:\n",
        "        return res[\"url\"]\n",
        "    else:\n",
        "        return res[\"msg\"]\n",
        "\n",
        "# ==============================================\n",
        "\n",
        "res = droplink(url)\n",
        "\n",
        "print(res)"
      ],
      "metadata": {
        "id": "611_HcrXfOOr",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter AppDrive or DriveApp etc. Look-Alike Link and as well as the Account Details (Required for Login Required Links only)**"
      ],
      "metadata": {
        "id": "WlRAIhcUoVHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown <center><img src='https://drfilestreambot.cf/AgADSA14799' height=\"85\">\n",
        "#@title <b><center>Enter App Drive or Drive App Link Below</center></b>\n",
        "print(\"Downloading Cloud-Scraper...\")\n",
        "!pip install cloudscraper\n",
        "import cloudscraper\n",
        "import re\n",
        "import requests\n",
        "from lxml import etree\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "\n",
        "# Website User Account (NOT GOOGLE ACCOUNT) ----\n",
        "url = \"\" #@param {type:\"string\"}\n",
        "Email = \"OPTIONAL\" #@param {type:\"string\"}\n",
        "Password = \"OPTIONAL\" #@param {type:\"string\"}\n",
        "\n",
        "'''\n",
        "NOTE:\n",
        " - Auto-detection for non-login urls, and indicated via 'link_type' (direct/login) in output.\n",
        "SUPPORTED DOMAINS:\n",
        " - appdrive.in\n",
        " - driveapp.in\n",
        " - drivehub.in\n",
        " - gdflix.pro\n",
        " - drivesharer.in\n",
        " - drivebit.in\n",
        " - drivelinks.in\n",
        " - driveace.in\n",
        " - drivepro.in\n",
        "\n",
        "'''\n",
        "print(\"Generating GDrive Link...\")\n",
        "\n",
        "# ===================================================================\n",
        "\n",
        "def unified(url):\n",
        "    try:\n",
        "        account = {\"email\": Email, \"passwd\": Password}\n",
        "        client = cloudscraper.create_scraper(allow_brotli=False)\n",
        "        client.headers.update(\n",
        "            {\n",
        "                \"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36\"\n",
        "            }\n",
        "        )\n",
        "        data = {\"email\": account[\"email\"], \"password\": account[\"passwd\"]}\n",
        "        client.post(f\"https://{urlparse(url).netloc}/login\", data=data)\n",
        "        res = client.get(url)\n",
        "        key = re.findall('\"key\",\\s+\"(.*?)\"', res.text)[0]\n",
        "        ddl_btn = etree.HTML(res.content).xpath(\"//button[@id='drc']\")\n",
        "        info = re.findall(\">(.*?)<\\/li>\", res.text)\n",
        "        info_parsed = {}\n",
        "        for item in info:\n",
        "            kv = [s.strip() for s in item.split(\":\", maxsplit=1)]\n",
        "            info_parsed[kv[0].lower()] = kv[1]\n",
        "        info_parsed = info_parsed\n",
        "        info_parsed[\"error\"] = False\n",
        "        info_parsed[\"link_type\"] = \"login\"\n",
        "        headers = {\n",
        "            \"Content-Type\": f\"multipart/form-data; boundary={'-'*4}_\",\n",
        "        }\n",
        "        data = {\"type\": 1, \"key\": key, \"action\": \"original\"}\n",
        "        if len(ddl_btn):\n",
        "            info_parsed[\"link_type\"] = \"direct\"\n",
        "            data[\"action\"] = \"direct\"\n",
        "        while data[\"type\"] <= 3:\n",
        "            boundary = f'{\"-\"*6}_'\n",
        "            data_string = \"\"\n",
        "            for item in data:\n",
        "                data_string += f\"{boundary}\\r\\n\"\n",
        "                data_string += f'Content-Disposition: form-data; name=\"{item}\"\\r\\n\\r\\n{data[item]}\\r\\n'\n",
        "            data_string += f\"{boundary}--\\r\\n\"\n",
        "            gen_payload = data_string\n",
        "            try:\n",
        "                response = client.post(url, data=gen_payload, headers=headers).json()\n",
        "                break\n",
        "            except BaseException:\n",
        "                data[\"type\"] += 1\n",
        "        if \"url\" in response:\n",
        "            info_parsed[\"gdrive_link\"] = response[\"url\"]\n",
        "        elif \"error\" in response and response[\"error\"]:\n",
        "            info_parsed[\"error\"] = True\n",
        "            info_parsed[\"error_message\"] = response[\"message\"]\n",
        "        else:\n",
        "            info_parsed[\"error\"] = True\n",
        "            info_parsed[\"error_message\"] = \"Something went wrong :(\"\n",
        "        if info_parsed[\"error\"]:\n",
        "            return info_parsed\n",
        "        if urlparse(url).netloc == \"driveapp.in\" and not info_parsed[\"error\"]:\n",
        "            res = client.get(info_parsed[\"gdrive_link\"])\n",
        "            drive_link = etree.HTML(res.content).xpath(\n",
        "                \"//a[contains(@class,'btn')]/@href\"\n",
        "            )[0]\n",
        "            info_parsed[\"gdrive_link\"] = drive_link\n",
        "        info_parsed[\"src_url\"] = url\n",
        "        if urlparse(url).netloc == \"drivehub.in\" and not info_parsed[\"error\"]:\n",
        "            res = client.get(info_parsed[\"gdrive_link\"])\n",
        "            drive_link = etree.HTML(res.content).xpath(\n",
        "                \"//a[contains(@class,'btn')]/@href\"\n",
        "            )[0]\n",
        "            info_parsed[\"gdrive_link\"] = drive_link\n",
        "        if urlparse(url).netloc == \"gdflix.pro\" and not info_parsed[\"error\"]:\n",
        "            res = client.get(info_parsed[\"gdrive_link\"])\n",
        "            drive_link = etree.HTML(res.content).xpath(\n",
        "                \"//a[contains(@class,'btn')]/@href\"\n",
        "            )[0]\n",
        "            info_parsed[\"gdrive_link\"] = drive_link\n",
        "\n",
        "        if urlparse(url).netloc == \"drivesharer.in\" and not info_parsed[\"error\"]:\n",
        "            res = client.get(info_parsed[\"gdrive_link\"])\n",
        "            drive_link = etree.HTML(res.content).xpath(\n",
        "                \"//a[contains(@class,'btn')]/@href\"\n",
        "            )[0]\n",
        "            info_parsed[\"gdrive_link\"] = drive_link\n",
        "        if urlparse(url).netloc == \"drivebit.in\" and not info_parsed[\"error\"]:\n",
        "            res = client.get(info_parsed[\"gdrive_link\"])\n",
        "            drive_link = etree.HTML(res.content).xpath(\n",
        "                \"//a[contains(@class,'btn')]/@href\"\n",
        "            )[0]\n",
        "            info_parsed[\"gdrive_link\"] = drive_link\n",
        "        if urlparse(url).netloc == \"drivelinks.in\" and not info_parsed[\"error\"]:\n",
        "            res = client.get(info_parsed[\"gdrive_link\"])\n",
        "            drive_link = etree.HTML(res.content).xpath(\n",
        "                \"//a[contains(@class,'btn')]/@href\"\n",
        "            )[0]\n",
        "            info_parsed[\"gdrive_link\"] = drive_link\n",
        "        if urlparse(url).netloc == \"driveace.in\" and not info_parsed[\"error\"]:\n",
        "            res = client.get(info_parsed[\"gdrive_link\"])\n",
        "            drive_link = etree.HTML(res.content).xpath(\n",
        "                \"//a[contains(@class,'btn')]/@href\"\n",
        "            )[0]\n",
        "            info_parsed[\"gdrive_link\"] = drive_link\n",
        "        if urlparse(url).netloc == \"drivepro.in\" and not info_parsed[\"error\"]:\n",
        "            res = client.get(info_parsed[\"gdrive_link\"])\n",
        "            drive_link = etree.HTML(res.content).xpath(\n",
        "                \"//a[contains(@class,'btn')]/@href\"\n",
        "            )[0]\n",
        "            info_parsed[\"gdrive_link\"] = drive_link\n",
        "        if info_parsed[\"error\"]:\n",
        "            return \"Faced an Unknown Error!\"\n",
        "        return info_parsed[\"gdrive_link\"]\n",
        "    except BaseException:\n",
        "        return \"Unable to Extract GDrive Link\"\n",
        "\n",
        "# ===================================================================\n",
        "\n",
        "res = unified(url)\n",
        "\n",
        "print(res)"
      ],
      "metadata": {
        "id": "0vqE8a8dm5T4",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter Linkvertise Link Below!**"
      ],
      "metadata": {
        "id": "OY1CtzT0pA8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown <center><img src='https://drfilestreambot.cf/AgADSQ14800' height=\"50\">\n",
        "#@title <b><center>Enter Linkvertise-Link Below</center></b>\n",
        "print(\"Downloading Cloud-Scraper...\")\n",
        "!pip install cloudscraper\n",
        "print(\"Importing Files!\")\n",
        "import cloudscraper\n",
        "\n",
        "url = \"\" #@param {type:\"string\"}\n",
        "print(\"You have Entered:\")\n",
        "print(\"URL:\")\n",
        "print(url)\n",
        "print(\"Bypassing the Link...\")\n",
        "# -------------------------------------------\n",
        "\n",
        "def linkvertise(url):\n",
        "    api = \"https://api.emilyx.in/api\"\n",
        "    client = cloudscraper.create_scraper(allow_brotli=False)\n",
        "    resp = client.get(url)\n",
        "    if resp.status_code == 404:\n",
        "        return \"File not found/The link you entered is wrong!\"\n",
        "    try:\n",
        "        resp = client.post(api, json={\"type\": \"linkvertise\", \"url\": url})\n",
        "        res = resp.json()\n",
        "    except BaseException:\n",
        "        return \"API UnResponsive / Invalid Link !\"\n",
        "    if res[\"success\"] is True:\n",
        "        return res[\"url\"]\n",
        "    else:\n",
        "        return res[\"msg\"]\n",
        "\n",
        "# -------------------------------------------\n",
        "\n",
        "res = linkvertise(url)\n",
        "\n",
        "print(res)\n",
        "print(\"Successfully Bypassed!\")"
      ],
      "metadata": {
        "id": "4yV_DpjXpBXj",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter RockLinks Link Below!**"
      ],
      "metadata": {
        "id": "IVaFnZycUqSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown <center><img src='https://drfilestreambot.cf/AgADSw14802' height=\"70\">\n",
        "#@title <b><center>Enter Rocklinks-Link Below</center></b>\n",
        "print(\"Downloading Cloud-Scraper...\")\n",
        "!pip install cloudscraper\n",
        "print(\"Setting Up!\")\n",
        "print(\"Performing Check...\")\n",
        "import time\n",
        "import cloudscraper\n",
        "from bs4 import BeautifulSoup\n",
        "print(\"Everything Looks Good! Lets Continue.\")\n",
        "\n",
        "url = \"\"  #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def bypass(url):\n",
        "    client = cloudscraper.create_scraper(allow_brotli=False)\n",
        "    if 'rocklinks.net' in url:\n",
        "        DOMAIN = \"https://blog.disheye.com\"\n",
        "    else:\n",
        "        DOMAIN = \"https://rocklinks.net\"\n",
        "\n",
        "    url = url[:-1] if url[-1] == '/' else url\n",
        "\n",
        "    code = url.split(\"/\")[-1]\n",
        "    if 'rocklinks.net' in url:\n",
        "        final_url = f\"{DOMAIN}/{code}?quelle=\"\n",
        "    else:\n",
        "        final_url = f\"{DOMAIN}/{code}\"\n",
        "\n",
        "    resp = client.get(final_url)\n",
        "    soup = BeautifulSoup(resp.content, \"html.parser\")\n",
        "\n",
        "    try: inputs = soup.find(id=\"go-link\").find_all(name=\"input\")\n",
        "    except: return \"Incorrect Link\"\n",
        "\n",
        "    data = { input.get('name'): input.get('value') for input in inputs }\n",
        "\n",
        "    h = { \"x-requested-with\": \"XMLHttpRequest\" }\n",
        "\n",
        "    time.sleep(10)\n",
        "    r = client.post(f\"{DOMAIN}/links/go\", data=data, headers=h)\n",
        "    try:\n",
        "        return r.json()['url']\n",
        "    except: return \"Something went wrong :(\"\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------------------------------\n",
        "res = bypass(url)\n",
        "\n",
        "print(res)\n",
        "print(\"Successfully Bypassed!\")"
      ],
      "metadata": {
        "id": "bG5bvLAxUui3",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter Gyanilinks Link Below!**"
      ],
      "metadata": {
        "id": "hzP92k6eACgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown <center><img src='https://drfilestreambot.cf/AgADTw14808' height=\"50\">\n",
        "#@title <b><center>Enter gyanilinks Link Below</center></b>\n",
        "print(\"Downloading Cloud-Scraper...\")\n",
        "!pip install cloudscraper\n",
        "print(\"Setting Up!\")\n",
        "print(\"Performing Check...\")\n",
        "import time\n",
        "import cloudscraper\n",
        "from bs4 import BeautifulSoup\n",
        "print(\"Everything Looks Good! Lets Continue.\")\n",
        "\n",
        "url = \"\"  #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "'''\n",
        "NOTE:\n",
        "SUPPORTED DOMAINS:\n",
        " - gtlinks.me\n",
        "\n",
        "'''\n",
        "# ---------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def bypass(url):\n",
        "    client = cloudscraper.create_scraper(allow_brotli=False)\n",
        "    if 'gtlinks.me' in url:\n",
        "        DOMAIN = \"https://go.gyanitheme.com\"\n",
        "    else:\n",
        "        return \"Incorrect Link\"\n",
        "\n",
        "    url = url[:-1] if url[-1] == '/' else url\n",
        "\n",
        "    code = url.split(\"/\")[-1]\n",
        "\n",
        "    final_url = f\"{DOMAIN}/{code}\"\n",
        "\n",
        "    resp = client.get(final_url)\n",
        "    soup = BeautifulSoup(resp.content, \"html.parser\")\n",
        "\n",
        "    try: inputs = soup.find(id=\"go-link\").find_all(name=\"input\")\n",
        "    except: return \"Incorrect Link\"\n",
        "\n",
        "    data = { input.get('name'): input.get('value') for input in inputs }\n",
        "\n",
        "    h = { \"x-requested-with\": \"XMLHttpRequest\" }\n",
        "\n",
        "    time.sleep(5)\n",
        "    r = client.post(f\"{DOMAIN}/links/go\", data=data, headers=h)\n",
        "    try:\n",
        "        return r.json()['url']\n",
        "    except: return \"Something went wrong :(\"\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "res = bypass(url)\n",
        "\n",
        "print(res)\n",
        "print(\"Successfully Bypassed!\")"
      ],
      "metadata": {
        "id": "_iWf_OF9jOKq",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter Shortingly Link Below!**"
      ],
      "metadata": {
        "id": "ew-UlUfi_eSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown <center><img src='https://drfilestreambot.cf/AgADUA14809' height=\"50\">\n",
        "#@title <b><center>Enter shortingly Link Below</center></b>\n",
        "print(\"Downloading Cloud-Scraper...\")\n",
        "!pip install cloudscraper\n",
        "print(\"Setting Up!\")\n",
        "print(\"Performing Check...\")\n",
        "import time\n",
        "import cloudscraper\n",
        "from bs4 import BeautifulSoup\n",
        "print(\"Everything Looks Good! Lets Continue.\")\n",
        "\n",
        "url = \"\"  #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "'''\n",
        "NOTE:\n",
        "SUPPORTED DOMAINS:\n",
        " - shortingly.me\n",
        "\n",
        "'''\n",
        "# ---------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def bypass(url):\n",
        "    client = cloudscraper.create_scraper(allow_brotli=False)\n",
        "    if 'shortingly.me' in url:\n",
        "        DOMAIN = \"https://go.techyjeeshan.xyz\"\n",
        "    else:\n",
        "        return \"Incorrect Link\"\n",
        "\n",
        "    url = url[:-1] if url[-1] == '/' else url\n",
        "\n",
        "    code = url.split(\"/\")[-1]\n",
        "\n",
        "    final_url = f\"{DOMAIN}/{code}\"\n",
        "\n",
        "    resp = client.get(final_url)\n",
        "    soup = BeautifulSoup(resp.content, \"html.parser\")\n",
        "\n",
        "    try: inputs = soup.find(id=\"go-link\").find_all(name=\"input\")\n",
        "    except: return \"Incorrect Link\"\n",
        "\n",
        "    data = { input.get('name'): input.get('value') for input in inputs }\n",
        "\n",
        "    h = { \"x-requested-with\": \"XMLHttpRequest\" }\n",
        "\n",
        "    time.sleep(5)\n",
        "    r = client.post(f\"{DOMAIN}/links/go\", data=data, headers=h)\n",
        "    try:\n",
        "        return r.json()['url']\n",
        "    except: return \"Something went wrong :(\"\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "res = bypass(url)\n",
        "\n",
        "print(res)\n",
        "print(\"Successfully Bypassed!\")"
      ],
      "metadata": {
        "id": "VLQNAB35oAGZ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter ShareUs Link Below!**"
      ],
      "metadata": {
        "id": "xOd-WNFWhs6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b><center>Enter ShareUs Link Below</center></b>\n",
        "#@markdown <center><img src='https://drfilestreambot.cf/AgADUQ14810' height=\"50\">\n",
        "import requests\n",
        "\n",
        "url = \"https://shareus.in/?i=y3wWSo\" #@param {type:\"string\"}\n",
        "token = url.split(\"=\")[-1]\n",
        "\n",
        "bypassed_url = \"https://us-central1-my-apps-server.cloudfunctions.net/r?shortid=\"+ token\n",
        "response = requests.get(bypassed_url).text\n",
        "print(response)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "nMbSeTR8gVdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter PSA link Below!**"
      ],
      "metadata": {
        "id": "eUj7FRLCxT9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b><center>Enter PSA Link Below</center></b>\n",
        "#@markdown <center><img src='https://drfilestreambot.cf/AgADVA14814' height=\"70\">\n",
        "\n",
        "!pip install cloudscraper\n",
        "!pip install bs4\n",
        "\n",
        "import cloudscraper\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import re\n",
        "\n",
        "def try2link_bypass(url):\n",
        "\tclient = cloudscraper.create_scraper(allow_brotli=False)\n",
        "\n",
        "\turl = url[:-1] if url[-1] == '/' else url\n",
        "\n",
        "\tparams = (('d', int(time.time()) + (60 * 4)),)\n",
        "\tr = client.get(url, params=params, headers= {'Referer': 'https://newforex.online/'})\n",
        "\n",
        "\tsoup = BeautifulSoup(r.text, 'html.parser')\n",
        "\tinputs = soup.find(id=\"go-link\").find_all(name=\"input\")\n",
        "\tdata = { input.get('name'): input.get('value') for input in inputs }\n",
        "\ttime.sleep(7)\n",
        "\n",
        "\theaders = {'Host': 'try2link.com', 'X-Requested-With': 'XMLHttpRequest', 'Origin': 'https://try2link.com', 'Referer': url}\n",
        "\n",
        "\tbypassed_url = client.post('https://try2link.com/links/go', headers=headers,data=data)\n",
        "\treturn bypassed_url.json()[\"url\"]\n",
        "\n",
        "\n",
        "def try2link_scrape(url):\n",
        "\tclient = cloudscraper.create_scraper(allow_brotli=False)\n",
        "\th = {\n",
        "\t'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36',\n",
        "\t}\n",
        "\tres = client.get(url, cookies={}, headers=h)\n",
        "\turl = 'https://try2link.com/'+re.findall('try2link\\.com\\/(.*?) ', res.text)[0]\n",
        "\treturn try2link_bypass(url)\n",
        "\n",
        "\n",
        "def psa_bypasser(psa_url):\n",
        "    client = cloudscraper.create_scraper(allow_brotli=False)\n",
        "    r = client.get(psa_url)\n",
        "    soup = BeautifulSoup(r.text, \"html.parser\").find_all(class_=\"dropshadowboxes-drop-shadow dropshadowboxes-rounded-corners dropshadowboxes-inside-and-outside-shadow dropshadowboxes-lifted-both dropshadowboxes-effect-default\")\n",
        "    links = \"\"\n",
        "    for link in soup:\n",
        "        try:\n",
        "            exit_gate = link.a.get(\"href\")\n",
        "            links = links + try2link_scrape(exit_gate) + '\\n'\n",
        "        except: pass\n",
        "    return links\n",
        "\n",
        "url = \"https://psa.pm/movie/the-infernal-machine-2022/\" #@param {type:\"string\"}\n",
        "print(psa_bypasser(url))\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Mo7njiR0xCXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter OUO Link Below!**"
      ],
      "metadata": {
        "id": "0FUIhDVl0Xri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b><center>Enter OUO Link Below</center></b>\n",
        "#@markdown <center><img src='https://drfilestreambot.cf/AgADVQ14815' height=\"60\">\n",
        "\n",
        "!pip install bs4\n",
        "\n",
        "import requests\n",
        "import re\n",
        "from urllib.parse import urlparse\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# RECAPTCHA v3 BYPASS\n",
        "# code from https://github.com/xcscxr/Recaptcha-v3-bypass\n",
        "def RecaptchaV3(ANCHOR_URL):\n",
        "    url_base = 'https://www.google.com/recaptcha/'\n",
        "    post_data = \"v={}&reason=q&c={}&k={}&co={}\"\n",
        "    client = requests.Session()\n",
        "    client.headers.update({\n",
        "        'content-type': 'application/x-www-form-urlencoded'\n",
        "    })\n",
        "    matches = re.findall('([api2|enterprise]+)\\/anchor\\?(.*)', ANCHOR_URL)[0]\n",
        "    url_base += matches[0]+'/'\n",
        "    params = matches[1]\n",
        "    res = client.get(url_base+'anchor', params=params)\n",
        "    token = re.findall(r'\"recaptcha-token\" value=\"(.*?)\"', res.text)[0]\n",
        "    params = dict(pair.split('=') for pair in params.split('&'))\n",
        "    post_data = post_data.format(params[\"v\"], token, params[\"k\"], params[\"co\"])\n",
        "    res = client.post(url_base+'reload', params=f'k={params[\"k\"]}', data=post_data)\n",
        "    answer = re.findall(r'\"rresp\",\"(.*?)\"', res.text)[0]\n",
        "    return answer\n",
        "\n",
        "\n",
        "# code from https://github.com/xcscxr/ouo-bypass/\n",
        "ANCHOR_URL = 'https://www.google.com/recaptcha/api2/anchor?ar=1&k=6Lcr1ncUAAAAAH3cghg6cOTPGARa8adOf-y9zv2x&co=aHR0cHM6Ly9vdW8uaW86NDQz&hl=en&v=1B_yv3CBEV10KtI2HJ6eEXhJ&size=invisible&cb=4xnsug1vufyr'\n",
        "def ouo(url):\n",
        "    client = requests.Session()\n",
        "    tempurl = url.replace(\"ouo.press\", \"ouo.io\")\n",
        "    p = urlparse(tempurl)\n",
        "    id = tempurl.split('/')[-1]\n",
        "\n",
        "    res = client.get(tempurl)\n",
        "    next_url = f\"{p.scheme}://{p.hostname}/go/{id}\"\n",
        "\n",
        "    for _ in range(2):\n",
        "        if res.headers.get('Location'):\n",
        "            break\n",
        "        bs4 = BeautifulSoup(res.content, 'lxml')\n",
        "        inputs = bs4.form.findAll(\"input\", {\"name\": re.compile(r\"token$\")})\n",
        "        data = { input.get('name'): input.get('value') for input in inputs }\n",
        "\n",
        "        ans = RecaptchaV3(ANCHOR_URL)\n",
        "        data['x-token'] = ans\n",
        "        h = {\n",
        "            'content-type': 'application/x-www-form-urlencoded'\n",
        "        }\n",
        "        res = client.post(next_url, data=data, headers=h, allow_redirects=False)\n",
        "        next_url = f\"{p.scheme}://{p.hostname}/xreallcygo/{id}\"\n",
        "\n",
        "    return res.headers.get('Location')\n",
        "\n",
        "url = \"https://ouo.press/Zu7Vs5\" #@param {type:\"string\"}\n",
        "print(ouo(url))\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XNKe8fWY0XMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter FileCrypt Link Below!**"
      ],
      "metadata": {
        "id": "Eysjw_9wp8SA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b><center>Enter FileCrypt Link Below</center></b>\n",
        "#@markdown <center><img src='https://drfilestreambot.cf/AgADVw14818' height=\"50\">\n",
        "\n",
        "\n",
        "!pip install bs4\n",
        "!pip install cloudscraper\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import cloudscraper\n",
        "import json\n",
        "\n",
        "client = cloudscraper.create_scraper(allow_brotli=False)\n",
        "\n",
        "\n",
        "# by https://github.com/bipinkrish/filecrypt-bypass\n",
        "def getlinks(dlc):\n",
        "    headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:103.0) Gecko/20100101 Firefox/103.0',\n",
        "    'Accept': 'application/json, text/javascript, */*',\n",
        "    'Accept-Language': 'en-US,en;q=0.5',\n",
        "    # 'Accept-Encoding': 'gzip, deflate',\n",
        "    'X-Requested-With': 'XMLHttpRequest',\n",
        "    'Origin': 'http://dcrypt.it',\n",
        "    'Connection': 'keep-alive',\n",
        "    'Referer': 'http://dcrypt.it/',\n",
        "    }\n",
        "\n",
        "    data = {\n",
        "        'content': dlc,\n",
        "    }\n",
        "\n",
        "    response = client.post('http://dcrypt.it/decrypt/paste', headers=headers, data=data).json()[\"success\"][\"links\"]\n",
        "    links = \"\"\n",
        "    for link in response:\n",
        "        links = links + link + \"\\n\"\n",
        "    print(links)\n",
        "    return links\n",
        "\n",
        "\n",
        "# by https://github.com/bipinkrish/filecrypt-bypass\n",
        "def filecrypt(url):\n",
        "\n",
        "    headers = {\n",
        "    \"authority\": \"filecrypt.co\",\n",
        "    \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
        "    \"accept-language\": \"en-US,en;q=0.9\",\n",
        "    \"cache-control\": \"max-age=0\",\n",
        "    \"content-type\": \"application/x-www-form-urlencoded\",\n",
        "    \"dnt\": \"1\",\n",
        "    \"origin\": \"https://filecrypt.co\",\n",
        "    \"referer\": url,\n",
        "    \"sec-ch-ua\": '\"Google Chrome\";v=\"105\", \"Not)A;Brand\";v=\"8\", \"Chromium\";v=\"105\"',\n",
        "    \"sec-ch-ua-mobile\": \"?0\",\n",
        "    \"sec-ch-ua-platform\": \"Windows\",\n",
        "    \"sec-fetch-dest\": \"document\",\n",
        "    \"sec-fetch-mode\": \"navigate\",\n",
        "    \"sec-fetch-site\": \"same-origin\",\n",
        "    \"sec-fetch-user\": \"?1\",\n",
        "    \"upgrade-insecure-requests\": \"1\",\n",
        "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36\"\n",
        "    }\n",
        "\n",
        "\n",
        "    resp = client.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(resp.content, \"html.parser\")\n",
        "\n",
        "    buttons = soup.find_all(\"button\")\n",
        "    for ele in buttons:\n",
        "        line = ele.get(\"onclick\")\n",
        "        if line !=None and \"DownloadDLC\" in line:\n",
        "            dlclink = \"https://filecrypt.co/DLC/\" + line.split(\"DownloadDLC('\")[1].split(\"'\")[0] + \".html\"\n",
        "            break\n",
        "\n",
        "    resp = client.get(dlclink,headers=headers)\n",
        "    getlinks(resp.text)\n",
        "\n",
        "\n",
        "url= \"https://filecrypt.co/Container/73F6D9D43B.html\" #@param {type:\"string\"}\n",
        "filecrypt(url)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "s1aVxPqCp63Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter DriveFire Link and it's CRYPT Below!**"
      ],
      "metadata": {
        "id": "WLQhZQdxUpBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b><center>Enter DriveFire Link Below</center></b>\n",
        "#@markdown <center><img src='https://drfilestreambot.cf/AgADe715535' height=\"60\">\n",
        "\n",
        "import requests\n",
        "import re\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "\n",
        "def parse_info(res):\n",
        "    info_parsed = {}\n",
        "    title = re.findall('>(.*?)<\\/h4>', res.text)[0]\n",
        "    info_chunks = re.findall('>(.*?)<\\/td>', res.text)\n",
        "    info_parsed['title'] = title\n",
        "    for i in range(0, len(info_chunks), 2):\n",
        "        info_parsed[info_chunks[i]] = info_chunks[i+1]\n",
        "    return info_parsed\n",
        "\n",
        "def drivefire_dl(url, crypt):\n",
        "    client = requests.Session()\n",
        "    client.cookies.update({'crypt': crypt})\n",
        "\n",
        "    res = client.get(url)\n",
        "    info_parsed = parse_info(res)\n",
        "    info_parsed['error'] = False\n",
        "\n",
        "    up = urlparse(url)\n",
        "    req_url = f\"{up.scheme}://{up.netloc}/ajax.php?ajax=download\"\n",
        "\n",
        "    file_id = url.split('/')[-1]\n",
        "\n",
        "    data = { 'id': file_id }\n",
        "\n",
        "    headers = {\n",
        "        'x-requested-with': 'XMLHttpRequest'\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        res = client.post(req_url, headers=headers, data=data).json()['file']\n",
        "    except: return {'error': True, 'src_url': url}\n",
        "\n",
        "    decoded_id = res.rsplit('/', 1)[-1]\n",
        "    info_parsed = f\"https://drive.google.com/file/d/{decoded_id}\"\n",
        "    return info_parsed\n",
        "\n",
        "\n",
        "url = \"\" #@param {type:\"string\"}\n",
        "DRIVEFIRE_CRYPT = \"\" #@param {type:\"string\"}\n",
        "print(drivefire_dl(url))\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Xyhc3GBcUoTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter HubDrive Link and it's CRYPT Below!**"
      ],
      "metadata": {
        "id": "JqsWaAOyXec9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b><center>Enter HubDrive Link Below</center></b>\n",
        "#@markdown <center><img src='https://drfilestreambot.cf/AgADgL15537' height=\"60\">\n",
        "\n",
        "import requests\n",
        "import re\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "\n",
        "def parse_info(res):\n",
        "    info_parsed = {}\n",
        "    title = re.findall('>(.*?)<\\/h4>', res.text)[0]\n",
        "    info_chunks = re.findall('>(.*?)<\\/td>', res.text)\n",
        "    info_parsed['title'] = title\n",
        "    for i in range(0, len(info_chunks), 2):\n",
        "        info_parsed[info_chunks[i]] = info_chunks[i+1]\n",
        "    return info_parsed\n",
        "\n",
        "def hubdrive_dl(url):\n",
        "    client = requests.Session()\n",
        "    client.cookies.update({'crypt': crypt})\n",
        "\n",
        "    res = client.get(url)\n",
        "    info_parsed = parse_info(res)\n",
        "    info_parsed['error'] = False\n",
        "\n",
        "    up = urlparse(url)\n",
        "    req_url = f\"{up.scheme}://{up.netloc}/ajax.php?ajax=download\"\n",
        "\n",
        "    file_id = url.split('/')[-1]\n",
        "\n",
        "    data = { 'id': file_id }\n",
        "\n",
        "    headers = {\n",
        "        'x-requested-with': 'XMLHttpRequest'\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        res = client.post(req_url, headers=headers, data=data).json()['file']\n",
        "    except: return {'error': True, 'src_url': url}\n",
        "\n",
        "    gd_id = re.findall('gd=(.*)', res, re.DOTALL)[0]\n",
        "\n",
        "    info_parsed['gdrive_url'] = f\"https://drive.google.com/open?id={gd_id}\"\n",
        "    info_parsed['src_url'] = url\n",
        "\n",
        "    return info_parsed['gdrive_url']\n",
        "\n",
        "\n",
        "url = \"\" #@param {type:\"string\"}\n",
        "HUBDRIVE_CRYPT = \"\" #@param {type:\"string\"}\n",
        "print(hubdrive_dl(url))\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "yZngJVqcXdvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter KatDrive Link and it's CRYPT Below!**"
      ],
      "metadata": {
        "id": "YiwuJbjrZEYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b><center>Enter KatDrive Link Below</center></b>\n",
        "#@markdown <center><img src='https://drfilestreambot.cf/AgADgr15540' height=\"60\">\n",
        "\n",
        "import requests\n",
        "import re\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "\n",
        "def parse_info(res):\n",
        "    info_parsed = {}\n",
        "    title = re.findall('>(.*?)<\\/h4>', res.text)[0]\n",
        "    info_chunks = re.findall('>(.*?)<\\/td>', res.text)\n",
        "    info_parsed['title'] = title\n",
        "    for i in range(0, len(info_chunks), 2):\n",
        "        info_parsed[info_chunks[i]] = info_chunks[i+1]\n",
        "    return info_parsed\n",
        "\n",
        "def katdrive_dl(url):\n",
        "    client = requests.Session()\n",
        "    client.cookies.update({'crypt': crypt})\n",
        "\n",
        "    res = client.get(url)\n",
        "    info_parsed = parse_info(res)\n",
        "    info_parsed['error'] = False\n",
        "\n",
        "    up = urlparse(url)\n",
        "    req_url = f\"{up.scheme}://{up.netloc}/ajax.php?ajax=download\"\n",
        "\n",
        "    file_id = url.split('/')[-1]\n",
        "\n",
        "    data = { 'id': file_id }\n",
        "\n",
        "    headers = {\n",
        "        'x-requested-with': 'XMLHttpRequest'\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        res = client.post(req_url, headers=headers, data=data).json()['file']\n",
        "    except: return {'error': True, 'src_url': url}\n",
        "\n",
        "    gd_id = re.findall('gd=(.*)', res, re.DOTALL)[0]\n",
        "\n",
        "    info_parsed['gdrive_url'] = f\"https://drive.google.com/open?id={gd_id}\"\n",
        "    info_parsed['src_url'] = url\n",
        "\n",
        "    return info_parsed['gdrive_url']\n",
        "\n",
        "\n",
        "url = \"\" #@param {type:\"string\"}\n",
        "KATDRIVE_CRYPT = \"\" #@param {type:\"string\"}\n",
        "print(katdrive_dl(url))\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "VK3C4ip4ZDuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter Kolop Link and it's CRYPT Below!**"
      ],
      "metadata": {
        "id": "oMCnm8lfaSyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b><center>Enter Kolop Link Below</center></b>\n",
        "#@markdown <center><img src='https://drfilestreambot.cf/AgADhL15546' height=\"60\">\n",
        "\n",
        "import requests\n",
        "import re\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "\n",
        "def parse_info(res):\n",
        "    info_parsed = {}\n",
        "    title = re.findall('>(.*?)<\\/h4>', res.text)[0]\n",
        "    info_chunks = re.findall('>(.*?)<\\/td>', res.text)\n",
        "    info_parsed['title'] = title\n",
        "    for i in range(0, len(info_chunks), 2):\n",
        "        info_parsed[info_chunks[i]] = info_chunks[i+1]\n",
        "    return info_parsed\n",
        "\n",
        "def kolop_dl(url):\n",
        "    client = requests.Session()\n",
        "    client.cookies.update({'crypt': crypt})\n",
        "\n",
        "    res = client.get(url)\n",
        "    info_parsed = parse_info(res)\n",
        "    info_parsed['error'] = False\n",
        "\n",
        "    up = urlparse(url)\n",
        "    req_url = f\"{up.scheme}://{up.netloc}/ajax.php?ajax=download\"\n",
        "\n",
        "    file_id = url.split('/')[-1]\n",
        "\n",
        "    data = { 'id': file_id }\n",
        "\n",
        "    headers = {\n",
        "        'x-requested-with': 'XMLHttpRequest'\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        res = client.post(req_url, headers=headers, data=data).json()['file']\n",
        "    except: return {'error': True, 'src_url': url}\n",
        "\n",
        "    gd_id = re.findall('gd=(.*)', res, re.DOTALL)[0]\n",
        "\n",
        "    info_parsed['gdrive_url'] = f\"https://drive.google.com/open?id={gd_id}\"\n",
        "    info_parsed['src_url'] = url\n",
        "\n",
        "    return info_parsed['gdrive_url']\n",
        "\n",
        "\n",
        "url = \"\" #@param {type:\"string\"}\n",
        "KOLOP_CRYPT = \"\" #@param {type:\"string\"}\n",
        "print(kolop_dl(url))\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Rbdt0oomaR2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter Ola Movies Link Below!**"
      ],
      "metadata": {
        "id": "6h6YJrmzPrZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b><center>Enter Ola Movies Link Below</center></b>\n",
        "#@markdown <center><img src='https://drfilestreambot.cf/AgADYr21713' height=\"50\">\n",
        "\n",
        "!pip install cloudscraper bs4\n",
        "\n",
        "import cloudscraper\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "\n",
        "def try2link_bypass(url):\n",
        "\tclient = cloudscraper.create_scraper(allow_brotli=False)\n",
        "\n",
        "\turl = url[:-1] if url[-1] == '/' else url\n",
        "\n",
        "\tparams = (('d', int(time.time()) + (60 * 4)),)\n",
        "\tr = client.get(url, params=params, headers= {'Referer': 'https://newforex.online/'})\n",
        "\n",
        "\tsoup = BeautifulSoup(r.text, 'html.parser')\n",
        "\tinputs = soup.find(id=\"go-link\").find_all(name=\"input\")\n",
        "\tdata = { input.get('name'): input.get('value') for input in inputs }\n",
        "\ttime.sleep(7)\n",
        "\n",
        "\theaders = {'Host': 'try2link.com', 'X-Requested-With': 'XMLHttpRequest', 'Origin': 'https://try2link.com', 'Referer': url}\n",
        "\n",
        "\tbypassed_url = client.post('https://try2link.com/links/go', headers=headers,data=data)\n",
        "\treturn bypassed_url.json()[\"url\"]\n",
        "\n",
        "\n",
        "def rocklinksbyapss(url):\n",
        "    client = cloudscraper.create_scraper(allow_brotli=False)\n",
        "    if 'rocklinks.net' in url:\n",
        "        DOMAIN = \"https://blog.disheye.com\"\n",
        "    else:\n",
        "        DOMAIN = \"https://rocklinks.net\"\n",
        "\n",
        "    url = url[:-1] if url[-1] == '/' else url\n",
        "\n",
        "    code = url.split(\"/\")[-1]\n",
        "    if 'rocklinks.net' in url:\n",
        "        final_url = f\"{DOMAIN}/{code}?quelle=\"\n",
        "    else:\n",
        "        final_url = f\"{DOMAIN}/{code}\"\n",
        "\n",
        "    resp = client.get(final_url)\n",
        "    soup = BeautifulSoup(resp.content, \"html.parser\")\n",
        "\n",
        "    try: inputs = soup.find(id=\"go-link\").find_all(name=\"input\")\n",
        "    except: return \"Incorrect Link\"\n",
        "\n",
        "    data = { input.get('name'): input.get('value') for input in inputs }\n",
        "\n",
        "    h = { \"x-requested-with\": \"XMLHttpRequest\" }\n",
        "\n",
        "    time.sleep(10)\n",
        "    r = client.post(f\"{DOMAIN}/links/go\", data=data, headers=h)\n",
        "    try:\n",
        "        return r.json()['url']\n",
        "    except: return \"Something went wrong :(\"\n",
        "\n",
        "def olamovies(url):\n",
        "\n",
        "    print(\"this takes time, you might want to take a break.\")\n",
        "    headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:103.0) Gecko/20100101 Firefox/103.0',\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',\n",
        "            'Accept-Language': 'en-US,en;q=0.5',\n",
        "            'Referer': url,\n",
        "            'Alt-Used': 'olamovies.ink',\n",
        "            'Connection': 'keep-alive',\n",
        "            'Upgrade-Insecure-Requests': '1',\n",
        "            'Sec-Fetch-Dest': 'document',\n",
        "            'Sec-Fetch-Mode': 'navigate',\n",
        "            'Sec-Fetch-Site': 'same-origin',\n",
        "            'Sec-Fetch-User': '?1',\n",
        "        }\n",
        "\n",
        "    client = cloudscraper.create_scraper()\n",
        "    res = client.get(url)\n",
        "    soup = BeautifulSoup(res.text,\"html.parser\")\n",
        "    soup = soup.findAll(\"div\", class_=\"wp-block-button\")\n",
        "\n",
        "    outlist = []\n",
        "    for ele in soup:\n",
        "        outlist.append(ele.find(\"a\").get(\"href\"))\n",
        "\n",
        "    slist = []\n",
        "    for ele in outlist:\n",
        "        try:\n",
        "            key = ele.split(\"?key=\")[1].split(\"&id=\")[0].replace(\"%2B\",\"+\").replace(\"%3D\",\"=\").replace(\"%2F\",\"/\")\n",
        "            id = ele.split(\"&id=\")[1]\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        count = 3\n",
        "        params = { 'key': key, 'id': id}\n",
        "        soup = \"None\"\n",
        "        print(\"trying\",\"https://olamovies.ink/download/&key=\"+key+\"&id=\"+id)\n",
        "\n",
        "        while 'rocklinks.net' not in soup and \"try2link.com\" not in soup:\n",
        "            res = client.get(\"https://olamovies.ink/download/\", params=params, headers=headers)\n",
        "            soup = BeautifulSoup(res.text,\"html.parser\")\n",
        "            soup = soup.findAll(\"a\")[0].get(\"href\")\n",
        "            if soup != \"\":\n",
        "                if \"try2link.com\" in soup or 'rocklinks.net' in soup:\n",
        "                    print(\"added\", soup)\n",
        "                    slist.append(soup)\n",
        "                else:\n",
        "                    print(soup, \"not addded\")\n",
        "            else:\n",
        "                if count == 0:\n",
        "                    print('moving on')\n",
        "                    break\n",
        "                else:\n",
        "                    count -= 1\n",
        "                    print(\"retrying\")\n",
        "\n",
        "            print(\"waiting 10 secs\")\n",
        "            time.sleep(10)\n",
        "\n",
        "    #print(slist)\n",
        "    final = []\n",
        "    for ele in slist:\n",
        "        if \"rocklinks.net\" in ele:\n",
        "            final.append(rocklinksbyapss(ele))\n",
        "        elif \"try2link.com\" in ele:\n",
        "            final.append(try2link_bypass(ele))\n",
        "        else:\n",
        "            print(ele)\n",
        "    #print(final)\n",
        "    links = \"\"\n",
        "    for ele in final:\n",
        "        links = links + ele + \"\\n\"\n",
        "    print(\"Bypassed Links\")\n",
        "    print(links)\n",
        "    return links\n",
        "\n",
        "url = \"https://olamovies.ink/total-dhamaal-2019-hindi/\" #@param {type:\"string\"}\n",
        "olamovies(url)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Oj6VXKxuPqoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter xpshort.com or mdisk.website like Script Link Below!**\n",
        "\n"
      ],
      "metadata": {
        "id": "lmCkohWORyq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b><center>Enter Script Link Below</center></b>\n",
        "#@markdown <center><img src='https://drfilestreambot.cf/AgADY721717' height=\"100\">\n",
        "\n",
        "!pip install requests bs4\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import json\n",
        "import re\n",
        "\n",
        "def RecaptchaV3(ANCHOR_URL=\"https://www.google.com/recaptcha/api2/anchor?ar=1&k=6Lcr1ncUAAAAAH3cghg6cOTPGARa8adOf-y9zv2x&co=aHR0cHM6Ly9vdW8uaW86NDQz&hl=en&v=1B_yv3CBEV10KtI2HJ6eEXhJ&size=invisible&cb=4xnsug1vufyr\"):\n",
        "    url_base = 'https://www.google.com/recaptcha/'\n",
        "    post_data = \"v={}&reason=q&c={}&k={}&co={}\"\n",
        "    client = requests.Session()\n",
        "    client.headers.update({\n",
        "        'content-type': 'application/x-www-form-urlencoded'\n",
        "    })\n",
        "    matches = re.findall('([api2|enterprise]+)\\/anchor\\?(.*)', ANCHOR_URL)[0]\n",
        "    url_base += matches[0]+'/'\n",
        "    params = matches[1]\n",
        "    res = client.get(url_base+'anchor', params=params)\n",
        "    token = re.findall(r'\"recaptcha-token\" value=\"(.*?)\"', res.text)[0]\n",
        "    params = dict(pair.split('=') for pair in params.split('&'))\n",
        "    post_data = post_data.format(params[\"v\"], token, params[\"k\"], params[\"co\"])\n",
        "    res = client.post(url_base+'reload', params=f'k={params[\"k\"]}', data=post_data)\n",
        "    answer = re.findall(r'\"rresp\",\"(.*?)\"', res.text)[0]\n",
        "    return answer\n",
        "\n",
        "\n",
        "def getfinal(domain, url,sess):\n",
        "\n",
        "    #sess = requests.session()\n",
        "    res = sess.get(url)\n",
        "    soup = BeautifulSoup(res.text,\"html.parser\")\n",
        "    soup = soup.find(\"form\").findAll(\"input\")\n",
        "    datalist = []\n",
        "    for ele in soup:\n",
        "        datalist.append(ele.get(\"value\"))\n",
        "\n",
        "    data = {\n",
        "            '_method': datalist[0],\n",
        "            '_csrfToken': datalist[1],\n",
        "            'ad_form_data': datalist[2],\n",
        "            '_Token[fields]': datalist[3],\n",
        "            '_Token[unlocked]': datalist[4],\n",
        "        }\n",
        "\n",
        "    sess.headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:103.0) Gecko/20100101 Firefox/103.0',\n",
        "            'Accept': 'application/json, text/javascript, */*; q=0.01',\n",
        "            'Accept-Language': 'en-US,en;q=0.5',\n",
        "            'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',\n",
        "            'X-Requested-With': 'XMLHttpRequest',\n",
        "            'Origin': domain,\n",
        "            'Connection': 'keep-alive',\n",
        "            'Referer': url,\n",
        "            'Sec-Fetch-Dest': 'empty',\n",
        "            'Sec-Fetch-Mode': 'cors',\n",
        "            'Sec-Fetch-Site': 'same-origin',\n",
        "            }\n",
        "\n",
        "    print(\"waiting 10 secs\")\n",
        "    time.sleep(10) # important\n",
        "    response = sess.post(domain+'/links/go', data=data).json()\n",
        "    furl = response[\"url\"]\n",
        "    print(furl)\n",
        "    return furl\n",
        "\n",
        "\n",
        "def getfirst(url):\n",
        "\n",
        "    sess = requests.session()\n",
        "    res = sess.get(url)\n",
        "\n",
        "    soup = BeautifulSoup(res.text,\"html.parser\")\n",
        "    soup = soup.find(\"form\")\n",
        "    action = soup.get(\"action\")\n",
        "    soup = soup.findAll(\"input\")\n",
        "    datalist = []\n",
        "    for ele in soup:\n",
        "        datalist.append(ele.get(\"value\"))\n",
        "    sess.headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:103.0) Gecko/20100101 Firefox/103.0',\n",
        "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',\n",
        "        'Accept-Language': 'en-US,en;q=0.5',\n",
        "        'Origin': action,\n",
        "        'Connection': 'keep-alive',\n",
        "        'Referer': action,\n",
        "        'Upgrade-Insecure-Requests': '1',\n",
        "        'Sec-Fetch-Dest': 'document',\n",
        "        'Sec-Fetch-Mode': 'navigate',\n",
        "        'Sec-Fetch-Site': 'same-origin',\n",
        "        'Sec-Fetch-User': '?1',\n",
        "    }\n",
        "\n",
        "    data = {'newwpsafelink': datalist[1], \"g-recaptcha-response\": RecaptchaV3()}\n",
        "    response = sess.post(action, data=data)\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    soup = soup.findAll(\"div\", class_=\"wpsafe-bottom text-center\")\n",
        "    for ele in soup:\n",
        "        rurl = ele.find(\"a\").get(\"onclick\")[13:-12]\n",
        "\n",
        "    res = sess.get(rurl)\n",
        "    furl = res.url\n",
        "    print(furl)\n",
        "    return getfinal(f'https://{furl.split(\"/\")[-2]}/',furl,sess)\n",
        "\n",
        "url = \"https://xpshort.com/9DNKf\" #@param {type:\"string\"}\n",
        "# or https://link.mdisk.website/Pnms055v\n",
        "try:\n",
        "    getfirst(url)\n",
        "except:\n",
        "    sess = requests.session()\n",
        "    getfinal(f'https://{url.split(\"/\")[-2]}/',url, sess)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XRoFPMe-Rx7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DDL Generator**\n",
        "\n",
        "disk.yandex.com,\n",
        "mediafire.com,\n",
        "uptobox.com,\n",
        "osdn.net,\n",
        "github.com,\n",
        "hxfile.co,\n",
        "anonfiles.com,\n",
        "letsupload.io,\n",
        "1drv.ms(onedrive),\n",
        "pixeldrain.com,\n",
        "antfiles.com,\n",
        "streamtape.com,\n",
        "bayfiles.com,\n",
        "racaty.net,\n",
        "1fichier.com,\n",
        "solidfiles.com,\n",
        "krakenfiles.com,\n",
        "upload.ee,\n",
        "mdisk.me,\n",
        "wetransfer.com,\n",
        "gofile.io,\n",
        "dropbox.com,\n",
        "zippyshare.com,\n",
        "megaup.net,\n",
        "fembed.net, fembed.com, femax20.com, fcdn.stream, feurl.com, layarkacaxxi.icu, naniplay.nanime.in, naniplay.nanime.biz, naniplay.com, mm9842.com,\n",
        "sbembed.com, watchsb.com, streamsb.net, sbplay.org."
      ],
      "metadata": {
        "id": "Npkq2uQ8NRfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this is for upgrading python version\n",
        "\n",
        "!wget -O mini.sh https://repo.anaconda.com/miniconda/Miniconda3-py38_4.8.2-Linux-x86_64.sh\n",
        "!chmod +x mini.sh\n",
        "!bash ./mini.sh -b -f -p /usr/local\n",
        "!conda install -q -y jupyter\n",
        "!conda install -q -y google-colab -c conda-forge\n",
        "!python -m ipykernel install --name \"py38\" --user"
      ],
      "metadata": {
        "id": "mBO3cLx6c6VE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b75f657-0250-4f96-9570-44a5d4e5c2d6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-25 07:21:11--  https://repo.anaconda.com/miniconda/Miniconda3-py38_4.8.2-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.32.241, 104.16.191.158, 2606:4700::6810:bf9e, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.32.241|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 89817099 (86M) [application/x-sh]\n",
            "Saving to: mini.sh\n",
            "\n",
            "mini.sh             100%[===================>]  85.66M   293MB/s    in 0.3s    \n",
            "\n",
            "2024-10-25 07:21:12 (293 MB/s) - mini.sh saved [89817099/89817099]\n",
            "\n",
            "PREFIX=/usr/local\n",
            "Unpacking payload ...\n",
            "Collecting package metadata (current_repodata.json): - \b\bdone\n",
            "Solving environment: | \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - _libgcc_mutex==0.1=main\n",
            "    - asn1crypto==1.3.0=py38_0\n",
            "    - ca-certificates==2020.1.1=0\n",
            "    - certifi==2019.11.28=py38_0\n",
            "    - cffi==1.14.0=py38h2e261b9_0\n",
            "    - chardet==3.0.4=py38_1003\n",
            "    - conda-package-handling==1.6.0=py38h7b6447c_0\n",
            "    - conda==4.8.2=py38_0\n",
            "    - cryptography==2.8=py38h1ba5d50_0\n",
            "    - idna==2.8=py38_1000\n",
            "    - ld_impl_linux-64==2.33.1=h53a641e_7\n",
            "    - libedit==3.1.20181209=hc058e9b_0\n",
            "    - libffi==3.2.1=hd88cf55_4\n",
            "    - libgcc-ng==9.1.0=hdf63c60_0\n",
            "    - libstdcxx-ng==9.1.0=hdf63c60_0\n",
            "    - ncurses==6.2=he6710b0_0\n",
            "    - openssl==1.1.1d=h7b6447c_4\n",
            "    - pip==20.0.2=py38_1\n",
            "    - pycosat==0.6.3=py38h7b6447c_0\n",
            "    - pycparser==2.19=py_0\n",
            "    - pyopenssl==19.1.0=py38_0\n",
            "    - pysocks==1.7.1=py38_0\n",
            "    - python==3.8.1=h0371630_1\n",
            "    - readline==7.0=h7b6447c_5\n",
            "    - requests==2.22.0=py38_1\n",
            "    - ruamel_yaml==0.15.87=py38h7b6447c_0\n",
            "    - setuptools==45.2.0=py38_0\n",
            "    - six==1.14.0=py38_0\n",
            "    - sqlite==3.31.1=h7b6447c_0\n",
            "    - tk==8.6.8=hbc83047_0\n",
            "    - tqdm==4.42.1=py_0\n",
            "    - urllib3==1.25.8=py38_0\n",
            "    - wheel==0.34.2=py38_0\n",
            "    - xz==5.2.4=h14c3975_4\n",
            "    - yaml==0.1.7=had09818_2\n",
            "    - zlib==1.2.11=h7b6447c_3\n",
            "\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main\n",
            "  asn1crypto         pkgs/main/linux-64::asn1crypto-1.3.0-py38_0\n",
            "  ca-certificates    pkgs/main/linux-64::ca-certificates-2020.1.1-0\n",
            "  certifi            pkgs/main/linux-64::certifi-2019.11.28-py38_0\n",
            "  cffi               pkgs/main/linux-64::cffi-1.14.0-py38h2e261b9_0\n",
            "  chardet            pkgs/main/linux-64::chardet-3.0.4-py38_1003\n",
            "  conda              pkgs/main/linux-64::conda-4.8.2-py38_0\n",
            "  conda-package-han~ pkgs/main/linux-64::conda-package-handling-1.6.0-py38h7b6447c_0\n",
            "  cryptography       pkgs/main/linux-64::cryptography-2.8-py38h1ba5d50_0\n",
            "  idna               pkgs/main/linux-64::idna-2.8-py38_1000\n",
            "  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.33.1-h53a641e_7\n",
            "  libedit            pkgs/main/linux-64::libedit-3.1.20181209-hc058e9b_0\n",
            "  libffi             pkgs/main/linux-64::libffi-3.2.1-hd88cf55_4\n",
            "  libgcc-ng          pkgs/main/linux-64::libgcc-ng-9.1.0-hdf63c60_0\n",
            "  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-9.1.0-hdf63c60_0\n",
            "  ncurses            pkgs/main/linux-64::ncurses-6.2-he6710b0_0\n",
            "  openssl            pkgs/main/linux-64::openssl-1.1.1d-h7b6447c_4\n",
            "  pip                pkgs/main/linux-64::pip-20.0.2-py38_1\n",
            "  pycosat            pkgs/main/linux-64::pycosat-0.6.3-py38h7b6447c_0\n",
            "  pycparser          pkgs/main/noarch::pycparser-2.19-py_0\n",
            "  pyopenssl          pkgs/main/linux-64::pyopenssl-19.1.0-py38_0\n",
            "  pysocks            pkgs/main/linux-64::pysocks-1.7.1-py38_0\n",
            "  python             pkgs/main/linux-64::python-3.8.1-h0371630_1\n",
            "  readline           pkgs/main/linux-64::readline-7.0-h7b6447c_5\n",
            "  requests           pkgs/main/linux-64::requests-2.22.0-py38_1\n",
            "  ruamel_yaml        pkgs/main/linux-64::ruamel_yaml-0.15.87-py38h7b6447c_0\n",
            "  setuptools         pkgs/main/linux-64::setuptools-45.2.0-py38_0\n",
            "  six                pkgs/main/linux-64::six-1.14.0-py38_0\n",
            "  sqlite             pkgs/main/linux-64::sqlite-3.31.1-h7b6447c_0\n",
            "  tk                 pkgs/main/linux-64::tk-8.6.8-hbc83047_0\n",
            "  tqdm               pkgs/main/noarch::tqdm-4.42.1-py_0\n",
            "  urllib3            pkgs/main/linux-64::urllib3-1.25.8-py38_0\n",
            "  wheel              pkgs/main/linux-64::wheel-0.34.2-py38_0\n",
            "  xz                 pkgs/main/linux-64::xz-5.2.4-h14c3975_4\n",
            "  yaml               pkgs/main/linux-64::yaml-0.1.7-had09818_2\n",
            "  zlib               pkgs/main/linux-64::zlib-1.2.11-h7b6447c_3\n",
            "\n",
            "\n",
            "Preparing transaction: - \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Executing transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /usr/local\n",
            "Collecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.\n",
            "Solving environment: ...working... failed with repodata from current_repodata.json, will retry with next repodata source.\n",
            "Collecting package metadata (repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - jupyter\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    anyio-3.5.0                |   py38h06a4308_0         165 KB\n",
            "    argon2-cffi-21.3.0         |     pyhd3eb1b0_0          15 KB\n",
            "    argon2-cffi-bindings-21.2.0|   py38h7f8727e_0          33 KB\n",
            "    asttokens-2.0.5            |     pyhd3eb1b0_0          20 KB\n",
            "    attrs-24.2.0               |   py38h06a4308_0         149 KB\n",
            "    babel-2.11.0               |   py38h06a4308_0         6.8 MB\n",
            "    backcall-0.2.0             |     pyhd3eb1b0_0          13 KB\n",
            "    beautifulsoup4-4.12.3      |   py38h06a4308_0         218 KB\n",
            "    bleach-4.1.0               |     pyhd3eb1b0_0         123 KB\n",
            "    boltons-23.0.0             |   py38h06a4308_0         426 KB\n",
            "    ca-certificates-2024.9.24  |       h06a4308_0         130 KB\n",
            "    certifi-2024.8.30          |   py38h06a4308_0         162 KB\n",
            "    comm-0.2.1                 |   py38h06a4308_0          14 KB\n",
            "    conda-23.5.2               |   py38h06a4308_0         1.0 MB\n",
            "    dbus-1.13.18               |       hb2f20db_0         504 KB\n",
            "    debugpy-1.5.1              |   py38h295c915_0         1.7 MB\n",
            "    decorator-5.1.1            |     pyhd3eb1b0_0          12 KB\n",
            "    defusedxml-0.7.1           |     pyhd3eb1b0_0          23 KB\n",
            "    entrypoints-0.4            |   py38h06a4308_0          16 KB\n",
            "    executing-0.8.3            |     pyhd3eb1b0_0          18 KB\n",
            "    expat-2.4.4                |       h295c915_0         169 KB\n",
            "    fontconfig-2.13.0          |       h9420a91_0         227 KB\n",
            "    freetype-2.11.0            |       h70c0345_0         618 KB\n",
            "    glib-2.63.1                |       h5a9c865_0         2.9 MB\n",
            "    gst-plugins-base-1.14.0    |       hbbd80ab_1         4.8 MB\n",
            "    gstreamer-1.14.0           |       hb453b48_1         3.1 MB\n",
            "    icu-58.2                   |       he6710b0_3        10.5 MB\n",
            "    importlib-metadata-7.0.1   |   py38h06a4308_0          40 KB\n",
            "    importlib_resources-6.4.0  |   py38h06a4308_0          61 KB\n",
            "    ipykernel-6.19.2           |   py38hb070fc8_0         218 KB\n",
            "    ipython-8.12.2             |   py38h06a4308_0         1.1 MB\n",
            "    ipython_genutils-0.2.0     |     pyhd3eb1b0_1          27 KB\n",
            "    ipywidgets-8.1.2           |   py38h06a4308_0         198 KB\n",
            "    jedi-0.19.1                |   py38h06a4308_0         1.0 MB\n",
            "    jinja2-3.1.4               |   py38h06a4308_0         275 KB\n",
            "    jpeg-9e                    |       h7f8727e_0         240 KB\n",
            "    json5-0.9.6                |     pyhd3eb1b0_0          21 KB\n",
            "    jsonpatch-1.33             |   py38h06a4308_1          31 KB\n",
            "    jsonpointer-2.1            |     pyhd3eb1b0_0           9 KB\n",
            "    jsonschema-4.17.3          |   py38h06a4308_0         140 KB\n",
            "    jupyter-1.0.0              |   py38h06a4308_9           7 KB\n",
            "    jupyter_client-7.2.2       |   py38h06a4308_0         191 KB\n",
            "    jupyter_console-6.6.3      |   py38h06a4308_0          45 KB\n",
            "    jupyter_core-5.7.2         |   py38h06a4308_0          80 KB\n",
            "    jupyter_server-1.23.4      |   py38h06a4308_0         382 KB\n",
            "    jupyterlab-3.5.3           |   py38h06a4308_0         4.4 MB\n",
            "    jupyterlab_pygments-0.2.2  |   py38h06a4308_0          17 KB\n",
            "    jupyterlab_server-2.16.3   |   py38h06a4308_0          80 KB\n",
            "    jupyterlab_widgets-3.0.10  |   py38h06a4308_0         194 KB\n",
            "    libpng-1.6.37              |       hbc83047_0         278 KB\n",
            "    libsodium-1.0.18           |       h7b6447c_0         244 KB\n",
            "    libuuid-1.0.3              |       h7f8727e_2          17 KB\n",
            "    libxcb-1.15                |       h7f8727e_0         505 KB\n",
            "    libxml2-2.9.9              |       hea5a465_1         1.6 MB\n",
            "    markupsafe-2.1.1           |   py38h7f8727e_0          21 KB\n",
            "    matplotlib-inline-0.1.6    |   py38h06a4308_0          16 KB\n",
            "    mistune-2.0.4              |   py38h06a4308_0          93 KB\n",
            "    nbclassic-1.1.0            |   py38h06a4308_0         6.3 MB\n",
            "    nbclient-0.8.0             |   py38h06a4308_0          97 KB\n",
            "    nbconvert-7.16.4           |   py38h06a4308_0         470 KB\n",
            "    nbformat-5.10.4            |   py38h06a4308_0         138 KB\n",
            "    nest-asyncio-1.6.0         |   py38h06a4308_0          14 KB\n",
            "    notebook-6.5.7             |   py38h06a4308_0         538 KB\n",
            "    notebook-shim-0.2.3        |   py38h06a4308_0          22 KB\n",
            "    openssl-1.1.1w             |       h7f8727e_0         3.7 MB\n",
            "    packaging-24.1             |   py38h06a4308_0         147 KB\n",
            "    pandocfilters-1.5.0        |     pyhd3eb1b0_0          11 KB\n",
            "    parso-0.8.3                |     pyhd3eb1b0_0          70 KB\n",
            "    pcre-8.45                  |       h295c915_0         207 KB\n",
            "    pexpect-4.8.0              |     pyhd3eb1b0_3          53 KB\n",
            "    pickleshare-0.7.5          |  pyhd3eb1b0_1003          13 KB\n",
            "    pkgutil-resolve-name-1.3.10|   py38h06a4308_1          10 KB\n",
            "    platformdirs-3.10.0        |   py38h06a4308_0          33 KB\n",
            "    pluggy-1.0.0               |   py38h06a4308_1          28 KB\n",
            "    prometheus_client-0.14.1   |   py38h06a4308_0          90 KB\n",
            "    prompt-toolkit-3.0.43      |   py38h06a4308_0         571 KB\n",
            "    prompt_toolkit-3.0.43      |       hd3eb1b0_0           5 KB\n",
            "    psutil-5.8.0               |   py38h27cfd23_1         327 KB\n",
            "    ptyprocess-0.7.0           |     pyhd3eb1b0_2          17 KB\n",
            "    pure_eval-0.2.2            |     pyhd3eb1b0_0          14 KB\n",
            "    pygments-2.15.1            |   py38h06a4308_1         1.8 MB\n",
            "    pyqt-5.9.2                 |   py38h05f1152_4         4.5 MB\n",
            "    pyrsistent-0.18.0          |   py38heee7806_0          94 KB\n",
            "    python-dateutil-2.9.0post0 |   py38h06a4308_2         279 KB\n",
            "    python-fastjsonschema-2.16.2|   py38h06a4308_0         230 KB\n",
            "    pytz-2024.1                |   py38h06a4308_0         213 KB\n",
            "    pyzmq-22.3.0               |   py38h295c915_2         476 KB\n",
            "    qt-5.9.7                   |       h5867ecd_1        68.5 MB\n",
            "    qtconsole-5.6.0            |   py38h06a4308_0         207 KB\n",
            "    qtpy-2.4.1                 |   py38h06a4308_0         108 KB\n",
            "    ruamel.yaml-0.16.12        |   py38h7b6447c_1         174 KB\n",
            "    ruamel.yaml.clib-0.2.6     |   py38h7f8727e_0         137 KB\n",
            "    send2trash-1.8.2           |   py38h06a4308_0          26 KB\n",
            "    sip-4.19.13                |   py38h295c915_0         279 KB\n",
            "    sniffio-1.3.0              |   py38h06a4308_0          15 KB\n",
            "    soupsieve-2.5              |   py38h06a4308_0          69 KB\n",
            "    stack_data-0.2.0           |     pyhd3eb1b0_0          22 KB\n",
            "    terminado-0.17.1           |   py38h06a4308_0          31 KB\n",
            "    tinycss2-1.2.1             |   py38h06a4308_0          40 KB\n",
            "    tomli-2.0.1                |   py38h06a4308_0          24 KB\n",
            "    toolz-0.12.0               |   py38h06a4308_0         105 KB\n",
            "    tornado-6.1                |   py38h27cfd23_0         588 KB\n",
            "    traitlets-5.14.3           |   py38h06a4308_0         179 KB\n",
            "    typing-extensions-4.11.0   |   py38h06a4308_0           9 KB\n",
            "    typing_extensions-4.11.0   |   py38h06a4308_0          59 KB\n",
            "    wcwidth-0.2.5              |     pyhd3eb1b0_0          26 KB\n",
            "    webencodings-0.5.1         |           py38_1          20 KB\n",
            "    websocket-client-1.8.0     |   py38h06a4308_0          90 KB\n",
            "    widgetsnbextension-4.0.10  |   py38h06a4308_0         947 KB\n",
            "    zeromq-4.3.4               |       h2531618_0         331 KB\n",
            "    zipp-3.20.2                |   py38h06a4308_0          25 KB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       137.6 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  anyio              pkgs/main/linux-64::anyio-3.5.0-py38h06a4308_0\n",
            "  argon2-cffi        pkgs/main/noarch::argon2-cffi-21.3.0-pyhd3eb1b0_0\n",
            "  argon2-cffi-bindi~ pkgs/main/linux-64::argon2-cffi-bindings-21.2.0-py38h7f8727e_0\n",
            "  asttokens          pkgs/main/noarch::asttokens-2.0.5-pyhd3eb1b0_0\n",
            "  attrs              pkgs/main/linux-64::attrs-24.2.0-py38h06a4308_0\n",
            "  babel              pkgs/main/linux-64::babel-2.11.0-py38h06a4308_0\n",
            "  backcall           pkgs/main/noarch::backcall-0.2.0-pyhd3eb1b0_0\n",
            "  beautifulsoup4     pkgs/main/linux-64::beautifulsoup4-4.12.3-py38h06a4308_0\n",
            "  bleach             pkgs/main/noarch::bleach-4.1.0-pyhd3eb1b0_0\n",
            "  boltons            pkgs/main/linux-64::boltons-23.0.0-py38h06a4308_0\n",
            "  comm               pkgs/main/linux-64::comm-0.2.1-py38h06a4308_0\n",
            "  dbus               pkgs/main/linux-64::dbus-1.13.18-hb2f20db_0\n",
            "  debugpy            pkgs/main/linux-64::debugpy-1.5.1-py38h295c915_0\n",
            "  decorator          pkgs/main/noarch::decorator-5.1.1-pyhd3eb1b0_0\n",
            "  defusedxml         pkgs/main/noarch::defusedxml-0.7.1-pyhd3eb1b0_0\n",
            "  entrypoints        pkgs/main/linux-64::entrypoints-0.4-py38h06a4308_0\n",
            "  executing          pkgs/main/noarch::executing-0.8.3-pyhd3eb1b0_0\n",
            "  expat              pkgs/main/linux-64::expat-2.4.4-h295c915_0\n",
            "  fontconfig         pkgs/main/linux-64::fontconfig-2.13.0-h9420a91_0\n",
            "  freetype           pkgs/main/linux-64::freetype-2.11.0-h70c0345_0\n",
            "  glib               pkgs/main/linux-64::glib-2.63.1-h5a9c865_0\n",
            "  gst-plugins-base   pkgs/main/linux-64::gst-plugins-base-1.14.0-hbbd80ab_1\n",
            "  gstreamer          pkgs/main/linux-64::gstreamer-1.14.0-hb453b48_1\n",
            "  icu                pkgs/main/linux-64::icu-58.2-he6710b0_3\n",
            "  importlib-metadata pkgs/main/linux-64::importlib-metadata-7.0.1-py38h06a4308_0\n",
            "  importlib_resourc~ pkgs/main/linux-64::importlib_resources-6.4.0-py38h06a4308_0\n",
            "  ipykernel          pkgs/main/linux-64::ipykernel-6.19.2-py38hb070fc8_0\n",
            "  ipython            pkgs/main/linux-64::ipython-8.12.2-py38h06a4308_0\n",
            "  ipython_genutils   pkgs/main/noarch::ipython_genutils-0.2.0-pyhd3eb1b0_1\n",
            "  ipywidgets         pkgs/main/linux-64::ipywidgets-8.1.2-py38h06a4308_0\n",
            "  jedi               pkgs/main/linux-64::jedi-0.19.1-py38h06a4308_0\n",
            "  jinja2             pkgs/main/linux-64::jinja2-3.1.4-py38h06a4308_0\n",
            "  jpeg               pkgs/main/linux-64::jpeg-9e-h7f8727e_0\n",
            "  json5              pkgs/main/noarch::json5-0.9.6-pyhd3eb1b0_0\n",
            "  jsonpatch          pkgs/main/linux-64::jsonpatch-1.33-py38h06a4308_1\n",
            "  jsonpointer        pkgs/main/noarch::jsonpointer-2.1-pyhd3eb1b0_0\n",
            "  jsonschema         pkgs/main/linux-64::jsonschema-4.17.3-py38h06a4308_0\n",
            "  jupyter            pkgs/main/linux-64::jupyter-1.0.0-py38h06a4308_9\n",
            "  jupyter_client     pkgs/main/linux-64::jupyter_client-7.2.2-py38h06a4308_0\n",
            "  jupyter_console    pkgs/main/linux-64::jupyter_console-6.6.3-py38h06a4308_0\n",
            "  jupyter_core       pkgs/main/linux-64::jupyter_core-5.7.2-py38h06a4308_0\n",
            "  jupyter_server     pkgs/main/linux-64::jupyter_server-1.23.4-py38h06a4308_0\n",
            "  jupyterlab         pkgs/main/linux-64::jupyterlab-3.5.3-py38h06a4308_0\n",
            "  jupyterlab_pygmen~ pkgs/main/linux-64::jupyterlab_pygments-0.2.2-py38h06a4308_0\n",
            "  jupyterlab_server  pkgs/main/linux-64::jupyterlab_server-2.16.3-py38h06a4308_0\n",
            "  jupyterlab_widgets pkgs/main/linux-64::jupyterlab_widgets-3.0.10-py38h06a4308_0\n",
            "  libpng             pkgs/main/linux-64::libpng-1.6.37-hbc83047_0\n",
            "  libsodium          pkgs/main/linux-64::libsodium-1.0.18-h7b6447c_0\n",
            "  libuuid            pkgs/main/linux-64::libuuid-1.0.3-h7f8727e_2\n",
            "  libxcb             pkgs/main/linux-64::libxcb-1.15-h7f8727e_0\n",
            "  libxml2            pkgs/main/linux-64::libxml2-2.9.9-hea5a465_1\n",
            "  markupsafe         pkgs/main/linux-64::markupsafe-2.1.1-py38h7f8727e_0\n",
            "  matplotlib-inline  pkgs/main/linux-64::matplotlib-inline-0.1.6-py38h06a4308_0\n",
            "  mistune            pkgs/main/linux-64::mistune-2.0.4-py38h06a4308_0\n",
            "  nbclassic          pkgs/main/linux-64::nbclassic-1.1.0-py38h06a4308_0\n",
            "  nbclient           pkgs/main/linux-64::nbclient-0.8.0-py38h06a4308_0\n",
            "  nbconvert          pkgs/main/linux-64::nbconvert-7.16.4-py38h06a4308_0\n",
            "  nbformat           pkgs/main/linux-64::nbformat-5.10.4-py38h06a4308_0\n",
            "  nest-asyncio       pkgs/main/linux-64::nest-asyncio-1.6.0-py38h06a4308_0\n",
            "  notebook           pkgs/main/linux-64::notebook-6.5.7-py38h06a4308_0\n",
            "  notebook-shim      pkgs/main/linux-64::notebook-shim-0.2.3-py38h06a4308_0\n",
            "  packaging          pkgs/main/linux-64::packaging-24.1-py38h06a4308_0\n",
            "  pandocfilters      pkgs/main/noarch::pandocfilters-1.5.0-pyhd3eb1b0_0\n",
            "  parso              pkgs/main/noarch::parso-0.8.3-pyhd3eb1b0_0\n",
            "  pcre               pkgs/main/linux-64::pcre-8.45-h295c915_0\n",
            "  pexpect            pkgs/main/noarch::pexpect-4.8.0-pyhd3eb1b0_3\n",
            "  pickleshare        pkgs/main/noarch::pickleshare-0.7.5-pyhd3eb1b0_1003\n",
            "  pkgutil-resolve-n~ pkgs/main/linux-64::pkgutil-resolve-name-1.3.10-py38h06a4308_1\n",
            "  platformdirs       pkgs/main/linux-64::platformdirs-3.10.0-py38h06a4308_0\n",
            "  pluggy             pkgs/main/linux-64::pluggy-1.0.0-py38h06a4308_1\n",
            "  prometheus_client  pkgs/main/linux-64::prometheus_client-0.14.1-py38h06a4308_0\n",
            "  prompt-toolkit     pkgs/main/linux-64::prompt-toolkit-3.0.43-py38h06a4308_0\n",
            "  prompt_toolkit     pkgs/main/noarch::prompt_toolkit-3.0.43-hd3eb1b0_0\n",
            "  psutil             pkgs/main/linux-64::psutil-5.8.0-py38h27cfd23_1\n",
            "  ptyprocess         pkgs/main/noarch::ptyprocess-0.7.0-pyhd3eb1b0_2\n",
            "  pure_eval          pkgs/main/noarch::pure_eval-0.2.2-pyhd3eb1b0_0\n",
            "  pygments           pkgs/main/linux-64::pygments-2.15.1-py38h06a4308_1\n",
            "  pyqt               pkgs/main/linux-64::pyqt-5.9.2-py38h05f1152_4\n",
            "  pyrsistent         pkgs/main/linux-64::pyrsistent-0.18.0-py38heee7806_0\n",
            "  python-dateutil    pkgs/main/linux-64::python-dateutil-2.9.0post0-py38h06a4308_2\n",
            "  python-fastjsonsc~ pkgs/main/linux-64::python-fastjsonschema-2.16.2-py38h06a4308_0\n",
            "  pytz               pkgs/main/linux-64::pytz-2024.1-py38h06a4308_0\n",
            "  pyzmq              pkgs/main/linux-64::pyzmq-22.3.0-py38h295c915_2\n",
            "  qt                 pkgs/main/linux-64::qt-5.9.7-h5867ecd_1\n",
            "  qtconsole          pkgs/main/linux-64::qtconsole-5.6.0-py38h06a4308_0\n",
            "  qtpy               pkgs/main/linux-64::qtpy-2.4.1-py38h06a4308_0\n",
            "  ruamel.yaml        pkgs/main/linux-64::ruamel.yaml-0.16.12-py38h7b6447c_1\n",
            "  ruamel.yaml.clib   pkgs/main/linux-64::ruamel.yaml.clib-0.2.6-py38h7f8727e_0\n",
            "  send2trash         pkgs/main/linux-64::send2trash-1.8.2-py38h06a4308_0\n",
            "  sip                pkgs/main/linux-64::sip-4.19.13-py38h295c915_0\n",
            "  sniffio            pkgs/main/linux-64::sniffio-1.3.0-py38h06a4308_0\n",
            "  soupsieve          pkgs/main/linux-64::soupsieve-2.5-py38h06a4308_0\n",
            "  stack_data         pkgs/main/noarch::stack_data-0.2.0-pyhd3eb1b0_0\n",
            "  terminado          pkgs/main/linux-64::terminado-0.17.1-py38h06a4308_0\n",
            "  tinycss2           pkgs/main/linux-64::tinycss2-1.2.1-py38h06a4308_0\n",
            "  tomli              pkgs/main/linux-64::tomli-2.0.1-py38h06a4308_0\n",
            "  toolz              pkgs/main/linux-64::toolz-0.12.0-py38h06a4308_0\n",
            "  tornado            pkgs/main/linux-64::tornado-6.1-py38h27cfd23_0\n",
            "  traitlets          pkgs/main/linux-64::traitlets-5.14.3-py38h06a4308_0\n",
            "  typing-extensions  pkgs/main/linux-64::typing-extensions-4.11.0-py38h06a4308_0\n",
            "  typing_extensions  pkgs/main/linux-64::typing_extensions-4.11.0-py38h06a4308_0\n",
            "  wcwidth            pkgs/main/noarch::wcwidth-0.2.5-pyhd3eb1b0_0\n",
            "  webencodings       pkgs/main/linux-64::webencodings-0.5.1-py38_1\n",
            "  websocket-client   pkgs/main/linux-64::websocket-client-1.8.0-py38h06a4308_0\n",
            "  widgetsnbextension pkgs/main/linux-64::widgetsnbextension-4.0.10-py38h06a4308_0\n",
            "  zeromq             pkgs/main/linux-64::zeromq-4.3.4-h2531618_0\n",
            "  zipp               pkgs/main/linux-64::zipp-3.20.2-py38h06a4308_0\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates                                2020.1.1-0 --> 2024.9.24-h06a4308_0\n",
            "  certifi                                 2019.11.28-py38_0 --> 2024.8.30-py38h06a4308_0\n",
            "  conda                                        4.8.2-py38_0 --> 23.5.2-py38h06a4308_0\n",
            "  openssl                                 1.1.1d-h7b6447c_4 --> 1.1.1w-h7f8727e_0\n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "Collecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - google-colab\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    _openmp_mutex-5.1          |            1_gnu          21 KB\n",
            "    aiohttp-3.8.1              |   py38h0a891b7_1         574 KB  conda-forge\n",
            "    aiosignal-1.3.1            |     pyhd8ed1ab_0          12 KB  conda-forge\n",
            "    async-timeout-4.0.3        |     pyhd8ed1ab_0          11 KB  conda-forge\n",
            "    bzip2-1.0.8                |       h7f98852_4         484 KB  conda-forge\n",
            "    ca-certificates-2024.8.30  |       hbcca054_0         155 KB  conda-forge\n",
            "    cachetools-5.5.0           |     pyhd8ed1ab_0          14 KB  conda-forge\n",
            "    certifi-2024.8.30          |     pyhd8ed1ab_0         160 KB  conda-forge\n",
            "    charset-normalizer-2.0.4   |     pyhd3eb1b0_0          35 KB\n",
            "    cryptography-43.0.0        |   py38hdda0065_0         2.2 MB\n",
            "    frozenlist-1.4.0           |   py38h5eee18b_0          52 KB\n",
            "    glib-2.78.4                |       h6a678d5_0         508 KB\n",
            "    glib-tools-2.78.4          |       h6a678d5_0         115 KB\n",
            "    google-auth-2.35.0         |     pyhff2d567_0         113 KB  conda-forge\n",
            "    google-colab-1.0.0         |     pyh44b312d_0          77 KB  conda-forge\n",
            "    gstreamer-1.14.1           |       h5eee18b_1         1.7 MB\n",
            "    jupyter-1.1.1              |     pyhd8ed1ab_0           9 KB  conda-forge\n",
            "    libblas-3.9.0              |       8_openblas          11 KB  conda-forge\n",
            "    libcblas-3.9.0             |       8_openblas          11 KB  conda-forge\n",
            "    libffi-3.4.2               |       h7f98852_5          57 KB  conda-forge\n",
            "    libgcc-ng-11.2.0           |       h1234567_1         5.3 MB\n",
            "    libgfortran-ng-7.5.0       |      h14aa051_20          23 KB  conda-forge\n",
            "    libgfortran4-7.5.0         |      h14aa051_20         1.2 MB  conda-forge\n",
            "    libglib-2.78.4             |       hdc74915_0         1.5 MB\n",
            "    libgomp-11.2.0             |       h1234567_1         474 KB\n",
            "    libiconv-1.17              |       h166bdaf_0         1.4 MB  conda-forge\n",
            "    liblapack-3.9.0            |       8_openblas          11 KB  conda-forge\n",
            "    libopenblas-0.3.12         |pthreads_hb3c22a3_1         8.2 MB  conda-forge\n",
            "    libstdcxx-ng-11.2.0        |       h1234567_1         4.7 MB\n",
            "    multidict-6.0.4            |   py38h5eee18b_0          54 KB\n",
            "    nbconvert-core-7.16.4      |     pyhd8ed1ab_0         185 KB  conda-forge\n",
            "    ncurses-6.4                |       h6a678d5_0         914 KB\n",
            "    numpy-1.22.3               |   py38h99721a1_2         6.8 MB  conda-forge\n",
            "    openssl-3.0.15             |       h5eee18b_0         5.2 MB\n",
            "    pandas-1.3.5               |   py38h43a58ef_0        13.0 MB  conda-forge\n",
            "    pcre2-10.42                |       hebb0a14_1         1.3 MB\n",
            "    portpicker-1.6.0           |     pyhd8ed1ab_0          20 KB  conda-forge\n",
            "    pyasn1-0.6.1               |     pyhd8ed1ab_1          61 KB  conda-forge\n",
            "    pyasn1-modules-0.4.1       |     pyhd8ed1ab_0          94 KB  conda-forge\n",
            "    pyopenssl-24.2.1           |     pyhd8ed1ab_2         125 KB  conda-forge\n",
            "    python-3.8.20              |       he870216_0        23.8 MB\n",
            "    python_abi-3.8             |           2_cp38           4 KB  conda-forge\n",
            "    pyu2f-0.1.5                |     pyhd8ed1ab_0          31 KB  conda-forge\n",
            "    readline-8.2               |       h5eee18b_0         357 KB\n",
            "    rsa-4.9                    |     pyhd8ed1ab_0          29 KB  conda-forge\n",
            "    sqlite-3.45.3              |       h5eee18b_0         1.2 MB\n",
            "    tk-8.6.14                  |       h39e8969_0         3.4 MB\n",
            "    xz-5.4.6                   |       h5eee18b_1         643 KB\n",
            "    yarl-1.7.2                 |   py38h0a891b7_2         133 KB  conda-forge\n",
            "    zlib-1.2.13                |       h5eee18b_1         111 KB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        86.5 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _openmp_mutex      pkgs/main/linux-64::_openmp_mutex-5.1-1_gnu \n",
            "  aiohttp            conda-forge/linux-64::aiohttp-3.8.1-py38h0a891b7_1 \n",
            "  aiosignal          conda-forge/noarch::aiosignal-1.3.1-pyhd8ed1ab_0 \n",
            "  async-timeout      conda-forge/noarch::async-timeout-4.0.3-pyhd8ed1ab_0 \n",
            "  bzip2              conda-forge/linux-64::bzip2-1.0.8-h7f98852_4 \n",
            "  cachetools         conda-forge/noarch::cachetools-5.5.0-pyhd8ed1ab_0 \n",
            "  charset-normalizer pkgs/main/noarch::charset-normalizer-2.0.4-pyhd3eb1b0_0 \n",
            "  frozenlist         pkgs/main/linux-64::frozenlist-1.4.0-py38h5eee18b_0 \n",
            "  glib-tools         pkgs/main/linux-64::glib-tools-2.78.4-h6a678d5_0 \n",
            "  google-auth        conda-forge/noarch::google-auth-2.35.0-pyhff2d567_0 \n",
            "  google-colab       conda-forge/noarch::google-colab-1.0.0-pyh44b312d_0 \n",
            "  libblas            conda-forge/linux-64::libblas-3.9.0-8_openblas \n",
            "  libcblas           conda-forge/linux-64::libcblas-3.9.0-8_openblas \n",
            "  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-7.5.0-h14aa051_20 \n",
            "  libgfortran4       conda-forge/linux-64::libgfortran4-7.5.0-h14aa051_20 \n",
            "  libglib            pkgs/main/linux-64::libglib-2.78.4-hdc74915_0 \n",
            "  libgomp            pkgs/main/linux-64::libgomp-11.2.0-h1234567_1 \n",
            "  libiconv           conda-forge/linux-64::libiconv-1.17-h166bdaf_0 \n",
            "  liblapack          conda-forge/linux-64::liblapack-3.9.0-8_openblas \n",
            "  libopenblas        conda-forge/linux-64::libopenblas-0.3.12-pthreads_hb3c22a3_1 \n",
            "  multidict          pkgs/main/linux-64::multidict-6.0.4-py38h5eee18b_0 \n",
            "  nbconvert-core     conda-forge/noarch::nbconvert-core-7.16.4-pyhd8ed1ab_0 \n",
            "  numpy              conda-forge/linux-64::numpy-1.22.3-py38h99721a1_2 \n",
            "  pandas             conda-forge/linux-64::pandas-1.3.5-py38h43a58ef_0 \n",
            "  pcre2              pkgs/main/linux-64::pcre2-10.42-hebb0a14_1 \n",
            "  portpicker         conda-forge/noarch::portpicker-1.6.0-pyhd8ed1ab_0 \n",
            "  pyasn1             conda-forge/noarch::pyasn1-0.6.1-pyhd8ed1ab_1 \n",
            "  pyasn1-modules     conda-forge/noarch::pyasn1-modules-0.4.1-pyhd8ed1ab_0 \n",
            "  python_abi         conda-forge/linux-64::python_abi-3.8-2_cp38 \n",
            "  pyu2f              conda-forge/noarch::pyu2f-0.1.5-pyhd8ed1ab_0 \n",
            "  rsa                conda-forge/noarch::rsa-4.9-pyhd8ed1ab_0 \n",
            "  yarl               conda-forge/linux-64::yarl-1.7.2-py38h0a891b7_2 \n",
            "\n",
            "The following packages will be REMOVED:\n",
            "\n",
            "  pyqt-5.9.2-py38h05f1152_4\n",
            "  qt-5.9.7-h5867ecd_1\n",
            "  qtconsole-5.6.0-py38h06a4308_0\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  cryptography                           2.8-py38h1ba5d50_0 --> 43.0.0-py38hdda0065_0 \n",
            "  glib                                    2.63.1-h5a9c865_0 --> 2.78.4-h6a678d5_0 \n",
            "  gstreamer                               1.14.0-hb453b48_1 --> 1.14.1-h5eee18b_1 \n",
            "  jupyter            pkgs/main/linux-64::jupyter-1.0.0-py3~ --> conda-forge/noarch::jupyter-1.1.1-pyhd8ed1ab_0 \n",
            "  libffi                 pkgs/main::libffi-3.2.1-hd88cf55_4 --> conda-forge::libffi-3.4.2-h7f98852_5 \n",
            "  libgcc-ng                                9.1.0-hdf63c60_0 --> 11.2.0-h1234567_1 \n",
            "  libstdcxx-ng                             9.1.0-hdf63c60_0 --> 11.2.0-h1234567_1 \n",
            "  ncurses                                    6.2-he6710b0_0 --> 6.4-h6a678d5_0 \n",
            "  openssl                                 1.1.1w-h7f8727e_0 --> 3.0.15-h5eee18b_0 \n",
            "  pyopenssl          pkgs/main/linux-64::pyopenssl-19.1.0-~ --> conda-forge/noarch::pyopenssl-24.2.1-pyhd8ed1ab_2 \n",
            "  python                                   3.8.1-h0371630_1 --> 3.8.20-he870216_0 \n",
            "  readline                                   7.0-h7b6447c_5 --> 8.2-h5eee18b_0 \n",
            "  sqlite                                  3.31.1-h7b6447c_0 --> 3.45.3-h5eee18b_0 \n",
            "  tk                                       8.6.8-hbc83047_0 --> 8.6.14-h39e8969_0 \n",
            "  xz                                       5.2.4-h14c3975_4 --> 5.4.6-h5eee18b_1 \n",
            "  zlib                                    1.2.11-h7b6447c_3 --> 1.2.13-h5eee18b_1 \n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  ca-certificates    pkgs/main::ca-certificates-2024.9.24-~ --> conda-forge::ca-certificates-2024.8.30-hbcca054_0 \n",
            "  certifi            pkgs/main/linux-64::certifi-2024.8.30~ --> conda-forge/noarch::certifi-2024.8.30-pyhd8ed1ab_0 \n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "Installed kernelspec py38 in /root/.local/share/jupyter/kernels/py38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b><center>Enter Link Below (Reload the web page and execute this cell) </center></b>\n",
        "#@markdown <center><img src='https://drfilestreambot.cf/AgADD716907' height=\"70\">\n",
        "\n",
        "!pip install requests lk21 cfscrape bs4 cloudscraper hashlib\n",
        "\n",
        "from requests import get as rget, head as rhead, post as rpost, Session as rsession\n",
        "from re import findall as re_findall, sub as re_sub, match as re_match, search as re_search\n",
        "from urllib.parse import urlparse, unquote\n",
        "from json import loads as jsonloads\n",
        "from lk21 import Bypass\n",
        "from cfscrape import create_scraper\n",
        "from bs4 import BeautifulSoup\n",
        "from base64 import standard_b64encode\n",
        "from time import sleep\n",
        "import cloudscraper\n",
        "import hashlib\n",
        "import requests\n",
        "\n",
        "class DirectDownloadLinkException(Exception):\n",
        "    \"\"\"Not method found for extracting direct download link from the http link\"\"\"\n",
        "    pass\n",
        "\n",
        "def yandex_disk(url: str) -> str:\n",
        "    \"\"\" Yandex.Disk direct link generator\n",
        "    Based on https://github.com/wldhx/yadisk-direct \"\"\"\n",
        "    try:\n",
        "        link = re_findall(r'\\b(https?://(yadi.sk|disk.yandex.com)\\S+)', url)[0][0]\n",
        "    except IndexError:\n",
        "        return \"No Yandex.Disk links found\\n\"\n",
        "    api = 'https://cloud-api.yandex.net/v1/disk/public/resources/download?public_key={}'\n",
        "    try:\n",
        "        return rget(api.format(link)).json()['href']\n",
        "    except KeyError:\n",
        "        raise DirectDownloadLinkException(\"ERROR: File not found/Download limit reached\")\n",
        "\n",
        "def uptobox(url: str, UPTOBOX_TOKEN: str) -> str:\n",
        "    \"\"\" Uptobox direct link generator\n",
        "    based on https://github.com/jovanzers/WinTenCermin and https://github.com/sinoobie/noobie-mirror \"\"\"\n",
        "    try:\n",
        "        link = re_findall(r'\\bhttps?://.*uptobox\\.com\\S+', url)[0]\n",
        "    except IndexError:\n",
        "        raise DirectDownloadLinkException(\"No Uptobox links found\")\n",
        "    if UPTOBOX_TOKEN is None:\n",
        "        print('UPTOBOX_TOKEN not provided!')\n",
        "        dl_url = link\n",
        "    else:\n",
        "        try:\n",
        "            link = re_findall(r'\\bhttp?://.*uptobox\\.com/dl\\S+', url)[0]\n",
        "            dl_url = link\n",
        "        except:\n",
        "            file_id = re_findall(r'\\bhttps?://.*uptobox\\.com/(\\w+)', url)[0]\n",
        "            file_link = f'https://uptobox.com/api/link?token={UPTOBOX_TOKEN}&file_code={file_id}'\n",
        "            req = rget(file_link)\n",
        "            result = req.json()\n",
        "            if result['message'].lower() == 'success':\n",
        "                dl_url = result['data']['dlLink']\n",
        "            elif result['message'].lower() == 'waiting needed':\n",
        "                waiting_time = result[\"data\"][\"waiting\"] + 1\n",
        "                waiting_token = result[\"data\"][\"waitingToken\"]\n",
        "                sleep(waiting_time)\n",
        "                req2 = rget(f\"{file_link}&waitingToken={waiting_token}\")\n",
        "                result2 = req2.json()\n",
        "                dl_url = result2['data']['dlLink']\n",
        "            elif result['message'].lower() == 'you need to wait before requesting a new download link':\n",
        "                cooldown = divmod(result['data']['waiting'], 60)\n",
        "                raise DirectDownloadLinkException(f\"ERROR: Uptobox is being limited please wait {cooldown[0]} min {cooldown[1]} sec.\")\n",
        "            else:\n",
        "                print(f\"UPTOBOX_ERROR: {result}\")\n",
        "                raise DirectDownloadLinkException(f\"ERROR: {result['message']}\")\n",
        "    return dl_url\n",
        "\n",
        "def mediafire(url: str) -> str:\n",
        "    \"\"\" MediaFire direct link generator \"\"\"\n",
        "    try:\n",
        "        link = re_findall(r'\\bhttps?://.*mediafire\\.com\\S+', url)[0]\n",
        "    except IndexError:\n",
        "        raise DirectDownloadLinkException(\"No MediaFire links found\")\n",
        "    page = BeautifulSoup(rget(link).content, 'html.parser')\n",
        "    info = page.find('a', {'aria-label': 'Download file'})\n",
        "    return info.get('href')\n",
        "\n",
        "def osdn(url: str) -> str:\n",
        "    \"\"\" OSDN direct link generator \"\"\"\n",
        "    osdn_link = 'https://osdn.net'\n",
        "    try:\n",
        "        link = re_findall(r'\\bhttps?://.*osdn\\.net\\S+', url)[0]\n",
        "    except IndexError:\n",
        "        raise DirectDownloadLinkException(\"No OSDN links found\")\n",
        "    page = BeautifulSoup(\n",
        "        rget(link, allow_redirects=True).content, 'html.parser')\n",
        "    info = page.find('a', {'class': 'mirror_link'})\n",
        "    link = unquote(osdn_link + info['href'])\n",
        "    mirrors = page.find('form', {'id': 'mirror-select-form'}).findAll('tr')\n",
        "    urls = []\n",
        "    for data in mirrors[1:]:\n",
        "        mirror = data.find('input')['value']\n",
        "        urls.append(re_sub(r'm=(.*)&f', f'm={mirror}&f', link))\n",
        "    return urls[0]\n",
        "\n",
        "def github(url: str) -> str:\n",
        "    \"\"\" GitHub direct links generator \"\"\"\n",
        "    try:\n",
        "        re_findall(r'\\bhttps?://.*github\\.com.*releases\\S+', url)[0]\n",
        "    except IndexError:\n",
        "        raise DirectDownloadLinkException(\"No GitHub Releases links found\")\n",
        "    download = rget(url, stream=True, allow_redirects=False)\n",
        "    try:\n",
        "        return download.headers[\"location\"]\n",
        "    except KeyError:\n",
        "        raise DirectDownloadLinkException(\"ERROR: Can't extract the link\")\n",
        "\n",
        "def hxfile(url: str) -> str:\n",
        "    \"\"\" Hxfile direct link generator\n",
        "    Based on https://github.com/zevtyardt/lk21\n",
        "    \"\"\"\n",
        "    return Bypass().bypass_filesIm(url)\n",
        "\n",
        "def anonfiles(url: str) -> str:\n",
        "    \"\"\" Anonfiles direct link generator\n",
        "    Based on https://github.com/zevtyardt/lk21\n",
        "    \"\"\"\n",
        "    return Bypass().bypass_anonfiles(url)\n",
        "\n",
        "def letsupload(url: str) -> str:\n",
        "    \"\"\" Letsupload direct link generator\n",
        "    Based on https://github.com/zevtyardt/lk21\n",
        "    \"\"\"\n",
        "    try:\n",
        "        link = re_findall(r'\\bhttps?://.*letsupload\\.io\\S+', url)[0]\n",
        "    except IndexError:\n",
        "        raise DirectDownloadLinkException(\"No Letsupload links found\\n\")\n",
        "    return Bypass().bypass_url(link)\n",
        "\n",
        "def fembed(link: str) -> str:\n",
        "    \"\"\" Fembed direct link generator\n",
        "    Based on https://github.com/zevtyardt/lk21\n",
        "    \"\"\"\n",
        "    dl_url= Bypass().bypass_fembed(link)\n",
        "    count = len(dl_url)\n",
        "    lst_link = [dl_url[i] for i in dl_url]\n",
        "    return lst_link[count-1]\n",
        "\n",
        "def sbembed(link: str) -> str:\n",
        "    \"\"\" Sbembed direct link generator\n",
        "    Based on https://github.com/zevtyardt/lk21\n",
        "    \"\"\"\n",
        "    dl_url= Bypass().bypass_sbembed(link)\n",
        "    count = len(dl_url)\n",
        "    lst_link = [dl_url[i] for i in dl_url]\n",
        "    return lst_link[count-1]\n",
        "\n",
        "def onedrive(link: str) -> str:\n",
        "    \"\"\" Onedrive direct link generator\n",
        "    Based on https://github.com/UsergeTeam/Userge \"\"\"\n",
        "    link_without_query = urlparse(link)._replace(query=None).geturl()\n",
        "    direct_link_encoded = str(standard_b64encode(bytes(link_without_query, \"utf-8\")), \"utf-8\")\n",
        "    direct_link1 = f\"https://api.onedrive.com/v1.0/shares/u!{direct_link_encoded}/root/content\"\n",
        "    resp = rhead(direct_link1)\n",
        "    if resp.status_code != 302:\n",
        "        raise DirectDownloadLinkException(\"ERROR: Unauthorized link, the link may be private\")\n",
        "    return resp.next.url\n",
        "\n",
        "def pixeldrain(url: str) -> str:\n",
        "    \"\"\" Based on https://github.com/yash-dk/TorToolkit-Telegram \"\"\"\n",
        "    url = url.strip(\"/ \")\n",
        "    file_id = url.split(\"/\")[-1]\n",
        "    if url.split(\"/\")[-2] == \"l\":\n",
        "        info_link = f\"https://pixeldrain.com/api/list/{file_id}\"\n",
        "        dl_link = f\"https://pixeldrain.com/api/list/{file_id}/zip\"\n",
        "    else:\n",
        "        info_link = f\"https://pixeldrain.com/api/file/{file_id}/info\"\n",
        "        dl_link = f\"https://pixeldrain.com/api/file/{file_id}\"\n",
        "    resp = rget(info_link).json()\n",
        "    if resp[\"success\"]:\n",
        "        return dl_link\n",
        "    else:\n",
        "        raise DirectDownloadLinkException(f\"ERROR: Cant't download due {resp['message']}.\")\n",
        "\n",
        "def antfiles(url: str) -> str:\n",
        "    \"\"\" Antfiles direct link generator\n",
        "    Based on https://github.com/zevtyardt/lk21\n",
        "    \"\"\"\n",
        "    return Bypass().bypass_antfiles(url)\n",
        "\n",
        "def streamtape(url: str) -> str:\n",
        "    \"\"\" Streamtape direct link generator\n",
        "    Based on https://github.com/zevtyardt/lk21\n",
        "    \"\"\"\n",
        "    return Bypass().bypass_streamtape(url)\n",
        "\n",
        "def racaty(url: str) -> str:\n",
        "    \"\"\" Racaty direct link generator\n",
        "    based on https://github.com/SlamDevs/slam-mirrorbot\"\"\"\n",
        "    dl_url = ''\n",
        "    try:\n",
        "        re_findall(r'\\bhttps?://.*racaty\\.net\\S+', url)[0]\n",
        "    except IndexError:\n",
        "        raise DirectDownloadLinkException(\"No Racaty links found\")\n",
        "    scraper = create_scraper()\n",
        "    r = scraper.get(url)\n",
        "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "    op = soup.find(\"input\", {\"name\": \"op\"})[\"value\"]\n",
        "    ids = soup.find(\"input\", {\"name\": \"id\"})[\"value\"]\n",
        "    rapost = scraper.post(url, data = {\"op\": op, \"id\": ids})\n",
        "    rsoup = BeautifulSoup(rapost.text, \"html.parser\")\n",
        "    dl_url = rsoup.find(\"a\", {\"id\": \"uniqueExpirylink\"})[\"href\"].replace(\" \", \"%20\")\n",
        "    return dl_url\n",
        "\n",
        "def fichier(link: str) -> str:\n",
        "    \"\"\" 1Fichier direct link generator\n",
        "    Based on https://github.com/Maujar\n",
        "    \"\"\"\n",
        "    regex = r\"^([http:\\/\\/|https:\\/\\/]+)?.*1fichier\\.com\\/\\?.+\"\n",
        "    gan = re_match(regex, link)\n",
        "    if not gan:\n",
        "      raise DirectDownloadLinkException(\"ERROR: The link you entered is wrong!\")\n",
        "    if \"::\" in link:\n",
        "      pswd = link.split(\"::\")[-1]\n",
        "      url = link.split(\"::\")[-2]\n",
        "    else:\n",
        "      pswd = None\n",
        "      url = link\n",
        "    try:\n",
        "      if pswd is None:\n",
        "        req = rpost(url)\n",
        "      else:\n",
        "        pw = {\"pass\": pswd}\n",
        "        req = rpost(url, data=pw)\n",
        "    except:\n",
        "      raise DirectDownloadLinkException(\"ERROR: Unable to reach 1fichier server!\")\n",
        "    if req.status_code == 404:\n",
        "      raise DirectDownloadLinkException(\"ERROR: File not found/The link you entered is wrong!\")\n",
        "    soup = BeautifulSoup(req.content, 'html.parser')\n",
        "    if soup.find(\"a\", {\"class\": \"ok btn-general btn-orange\"}) is not None:\n",
        "        dl_url = soup.find(\"a\", {\"class\": \"ok btn-general btn-orange\"})[\"href\"]\n",
        "        if dl_url is None:\n",
        "          raise DirectDownloadLinkException(\"ERROR: Unable to generate Direct Link 1fichier!\")\n",
        "        else:\n",
        "          return dl_url\n",
        "    elif len(soup.find_all(\"div\", {\"class\": \"ct_warn\"})) == 3:\n",
        "        str_2 = soup.find_all(\"div\", {\"class\": \"ct_warn\"})[-1]\n",
        "        if \"you must wait\" in str(str_2).lower():\n",
        "            numbers = [int(word) for word in str(str_2).split() if word.isdigit()]\n",
        "            if not numbers:\n",
        "                raise DirectDownloadLinkException(\"ERROR: 1fichier is on a limit. Please wait a few minutes/hour.\")\n",
        "            else:\n",
        "                raise DirectDownloadLinkException(f\"ERROR: 1fichier is on a limit. Please wait {numbers[0]} minute.\")\n",
        "        elif \"protect access\" in str(str_2).lower():\n",
        "          raise DirectDownloadLinkException(f\"ERROR: This link requires a password!\\n\\n<b>This link requires a password!</b>\\n- Insert sign <b>::</b> after the link and write the password after the sign.\\n\\n<b>Example:</b> https://1fichier.com/?smmtd8twfpm66awbqz04::love you\\n\\n* No spaces between the signs <b>::</b>\\n* For the password, you can use a space!\")\n",
        "        else:\n",
        "            print(str_2)\n",
        "            raise DirectDownloadLinkException(\"ERROR: Failed to generate Direct Link from 1fichier!\")\n",
        "    elif len(soup.find_all(\"div\", {\"class\": \"ct_warn\"})) == 4:\n",
        "        str_1 = soup.find_all(\"div\", {\"class\": \"ct_warn\"})[-2]\n",
        "        str_3 = soup.find_all(\"div\", {\"class\": \"ct_warn\"})[-1]\n",
        "        if \"you must wait\" in str(str_1).lower():\n",
        "            numbers = [int(word) for word in str(str_1).split() if word.isdigit()]\n",
        "            if not numbers:\n",
        "                raise DirectDownloadLinkException(\"ERROR: 1fichier is on a limit. Please wait a few minutes/hour.\")\n",
        "            else:\n",
        "                raise DirectDownloadLinkException(f\"ERROR: 1fichier is on a limit. Please wait {numbers[0]} minute.\")\n",
        "        elif \"bad password\" in str(str_3).lower():\n",
        "          raise DirectDownloadLinkException(\"ERROR: The password you entered is wrong!\")\n",
        "        else:\n",
        "            raise DirectDownloadLinkException(\"ERROR: Error trying to generate Direct Link from 1fichier!\")\n",
        "    else:\n",
        "        raise DirectDownloadLinkException(\"ERROR: Error trying to generate Direct Link from 1fichier!\")\n",
        "\n",
        "def solidfiles(url: str) -> str:\n",
        "    \"\"\" Solidfiles direct link generator\n",
        "    Based on https://github.com/Xonshiz/SolidFiles-Downloader\n",
        "    By https://github.com/Jusidama18 \"\"\"\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36'\n",
        "    }\n",
        "    pageSource = rget(url, headers = headers).text\n",
        "    mainOptions = str(re_search(r'viewerOptions\\'\\,\\ (.*?)\\)\\;', pageSource).group(1))\n",
        "    return jsonloads(mainOptions)[\"downloadUrl\"]\n",
        "\n",
        "def krakenfiles(page_link: str) -> str:\n",
        "    \"\"\" krakenfiles direct link generator\n",
        "    Based on https://github.com/tha23rd/py-kraken\n",
        "    By https://github.com/junedkh \"\"\"\n",
        "    page_resp = rsession().get(page_link)\n",
        "    soup = BeautifulSoup(page_resp.text, \"html.parser\")\n",
        "    try:\n",
        "        token = soup.find(\"input\", id=\"dl-token\")[\"value\"]\n",
        "    except:\n",
        "        raise DirectDownloadLinkException(f\"Page link is wrong: {page_link}\")\n",
        "\n",
        "    hashes = [\n",
        "        item[\"data-file-hash\"]\n",
        "        for item in soup.find_all(\"div\", attrs={\"data-file-hash\": True})\n",
        "    ]\n",
        "    if not hashes:\n",
        "        raise DirectDownloadLinkException(f\"ERROR: Hash not found for : {page_link}\")\n",
        "\n",
        "    dl_hash = hashes[0]\n",
        "\n",
        "    payload = f'------WebKitFormBoundary7MA4YWxkTrZu0gW\\r\\nContent-Disposition: form-data; name=\"token\"\\r\\n\\r\\n{token}\\r\\n------WebKitFormBoundary7MA4YWxkTrZu0gW--'\n",
        "    headers = {\n",
        "        \"content-type\": \"multipart/form-data; boundary=----WebKitFormBoundary7MA4YWxkTrZu0gW\",\n",
        "        \"cache-control\": \"no-cache\",\n",
        "        \"hash\": dl_hash,\n",
        "    }\n",
        "\n",
        "    dl_link_resp = rsession().post(\n",
        "        f\"https://krakenfiles.com/download/{hash}\", data=payload, headers=headers)\n",
        "\n",
        "    dl_link_json = dl_link_resp.json()\n",
        "\n",
        "    if \"url\" in dl_link_json:\n",
        "        return dl_link_json[\"url\"]\n",
        "    else:\n",
        "        raise DirectDownloadLinkException(f\"ERROR: Failed to acquire download URL from kraken for : {page_link}\")\n",
        "\n",
        "def uploadee(url: str) -> str:\n",
        "    \"\"\" uploadee direct link generator\n",
        "    By https://github.com/iron-heart-x\"\"\"\n",
        "    try:\n",
        "        soup = BeautifulSoup(rget(url).content, 'html.parser')\n",
        "        sa = soup.find('a', attrs={'id':'d_l'})\n",
        "        return sa['href']\n",
        "    except:\n",
        "        raise DirectDownloadLinkException(f\"ERROR: Failed to acquire download URL from upload.ee for : {url}\")\n",
        "\n",
        "def mdisk(url):\n",
        "    api = \"https://api.emilyx.in/api\"\n",
        "    client = cloudscraper.create_scraper(allow_brotli=False)\n",
        "    resp = client.get(url)\n",
        "    if resp.status_code == 404:\n",
        "        return \"File not found/The link you entered is wrong!\"\n",
        "    try:\n",
        "        resp = client.post(api, json={\"type\": \"mdisk\", \"url\": url})\n",
        "        res = resp.json()\n",
        "    except BaseException:\n",
        "        return \"API UnResponsive / Invalid Link !\"\n",
        "    if res[\"success\"] is True:\n",
        "        return res[\"url\"]\n",
        "    else:\n",
        "        return res[\"msg\"]\n",
        "\n",
        "def wetransfer(url):\n",
        "    api = \"https://api.emilyx.in/api\"\n",
        "    client = cloudscraper.create_scraper(allow_brotli=False)\n",
        "    resp = client.get(url)\n",
        "    if resp.status_code == 404:\n",
        "        return \"File not found/The link you entered is wrong!\"\n",
        "    try:\n",
        "        resp = client.post(api, json={\"type\": \"wetransfer\", \"url\": url})\n",
        "        res = resp.json()\n",
        "    except BaseException:\n",
        "        return \"API UnResponsive / Invalid Link !\"\n",
        "    if res[\"success\"] is True:\n",
        "        return res[\"url\"]\n",
        "    else:\n",
        "        return res[\"msg\"]\n",
        "\n",
        "def gofile_dl(url,password=\"\"):\n",
        "    api_uri = 'https://api.gofile.io'\n",
        "    client = requests.Session()\n",
        "    res = client.get(api_uri+'/createAccount').json()\n",
        "\n",
        "    data = {\n",
        "        'contentId': url.split('/')[-1],\n",
        "        'token': res['data']['token'],\n",
        "        'websiteToken': '12345',\n",
        "        'cache': 'true',\n",
        "        'password': hashlib.sha256(password.encode('utf-8')).hexdigest()\n",
        "    }\n",
        "    res = client.get(api_uri+'/getContent', params=data).json()\n",
        "\n",
        "    content = []\n",
        "    for item in res['data']['contents'].values():\n",
        "        content.append(item)\n",
        "\n",
        "    return {\n",
        "        'accountToken': data['token'],\n",
        "        'files': content\n",
        "    }[\"files\"][0][\"link\"]\n",
        "\n",
        "def dropbox(url):\n",
        "    return url.replace(\"www.\",\"\").replace(\"dropbox.com\",\"dl.dropboxusercontent.com\").replace(\"?dl=0\",\"\")\n",
        "\n",
        "def zippyshare(url):\n",
        "    resp = requests.get(url).text\n",
        "    surl = resp.split(\"document.getElementById('dlbutton').href = \")[1].split(\";\")[0]\n",
        "    parts = surl.split(\"(\")[1].split(\")\")[0].split(\" \")\n",
        "    val = str(int(parts[0]) % int(parts[2]) + int(parts[4]) % int(parts[6]))\n",
        "    surl = surl.split('\"')\n",
        "    burl = url.split(\"zippyshare.com\")[0]\n",
        "    furl = burl + \"zippyshare.com\" + surl[1] + val + surl[-2]\n",
        "    print(furl)\n",
        "\n",
        "def megaup(url):\n",
        "    api = \"https://api.emilyx.in/api\"\n",
        "    client = cloudscraper.create_scraper(allow_brotli=False)\n",
        "    resp = client.get(url)\n",
        "    if resp.status_code == 404:\n",
        "        return \"File not found/The link you entered is wrong!\"\n",
        "    try:\n",
        "        resp = client.post(api, json={\"type\": \"megaup\", \"url\": url})\n",
        "        res = resp.json()\n",
        "    except BaseException:\n",
        "        return \"API UnResponsive / Invalid Link !\"\n",
        "    if res[\"success\"] is True:\n",
        "        return res[\"url\"]\n",
        "    else:\n",
        "        return res[\"msg\"]\n",
        "\n",
        "def direct_link_generator(link: str):\n",
        "    \"\"\" direct links generator \"\"\"\n",
        "    if 'yadi.sk' in link or 'disk.yandex.com' in link:\n",
        "        return yandex_disk(link)\n",
        "    elif 'mediafire.com' in link:\n",
        "        return mediafire(link)\n",
        "    elif 'uptobox.com' in link:\n",
        "        return uptobox(link,UPTOBOX_TOKEN)\n",
        "    elif 'osdn.net' in link:\n",
        "        return osdn(link)\n",
        "    elif 'github.com' in link:\n",
        "        return github(link)\n",
        "    elif 'hxfile.co' in link:\n",
        "        return hxfile(link)\n",
        "    elif 'anonfiles.com' in link:\n",
        "        return anonfiles(link)\n",
        "    elif 'letsupload.io' in link:\n",
        "        return letsupload(link)\n",
        "    elif '1drv.ms' in link:\n",
        "        return onedrive(link)\n",
        "    elif 'pixeldrain.com' in link:\n",
        "        return pixeldrain(link)\n",
        "    elif 'antfiles.com' in link:\n",
        "        return antfiles(link)\n",
        "    elif 'streamtape.com' in link:\n",
        "        return streamtape(link)\n",
        "    elif 'bayfiles.com' in link:\n",
        "        return anonfiles(link)\n",
        "    elif 'racaty.net' in link:\n",
        "        return racaty(link)\n",
        "    elif '1fichier.com' in link:\n",
        "        return fichier(link)\n",
        "    elif 'solidfiles.com' in link:\n",
        "        return solidfiles(link)\n",
        "    elif 'krakenfiles.com' in link:\n",
        "        return krakenfiles(link)\n",
        "    elif 'upload.ee' in link:\n",
        "        return uploadee(link)\n",
        "    elif 'mdisk.me' in link:\n",
        "        return mdisk(link)\n",
        "    elif 'wetransfer.com' in link:\n",
        "        return wetransfer(link)\n",
        "    elif 'gofile.io' in link:\n",
        "        return gofile_dl(link,GO_FILE_PASS)\n",
        "    elif 'dropbox.com' in link:\n",
        "        return dropbox(link)\n",
        "    elif 'zippyshare.com' in link:\n",
        "        return zippyshare(link)\n",
        "    elif 'megaup.net' in link:\n",
        "        return megaup(link)\n",
        "    elif any(x in link for x in fmed_list):\n",
        "        return fembed(link)\n",
        "    elif any(x in link for x in ['sbembed.com', 'watchsb.com', 'streamsb.net', 'sbplay.org']):\n",
        "        return sbembed(link)\n",
        "    else:\n",
        "        raise DirectDownloadLinkException(f'No Direct link function found for {link}')\n",
        "\n",
        "\n",
        "supported_sites_list = \"disk.yandex.com\\nmediafire.com\\nuptobox.com\\nosdn.net\\ngithub.com\\nhxfile.co\\nanonfiles.com\\nletsupload.io\\n1drv.ms(onedrive)\\n\\\n",
        "pixeldrain.com\\nantfiles.com\\nstreamtape.com\\nbayfiles.com\\nracaty.net\\n1fichier.com\\nsolidfiles.com\\nkrakenfiles.com\\n\\\n",
        "upload.ee\\nmdisk.me\\nwetransfer.com\\ngofile.io\\ndropbox.com\\nzippyshare.com\\nmegaup.net\\n\\\n",
        "fembed.net, fembed.com, femax20.com, fcdn.stream, feurl.com, layarkacaxxi.icu, naniplay.nanime.in, naniplay.nanime.biz, naniplay.com, mm9842.com\\n\\\n",
        "sbembed.com, watchsb.com, streamsb.net, sbplay.org\"\n",
        "\n",
        "fmed_list = ['fembed.net', 'fembed.com', 'femax20.com', 'fcdn.stream', 'feurl.com', 'layarkacaxxi.icu',\n",
        "             'naniplay.nanime.in', 'naniplay.nanime.biz', 'naniplay.com', 'mm9842.com']\n",
        "\n",
        "url = \"https://pixeldrain.com/l/qrGbojLQ#item=6\" #@param {type:\"string\"}\n",
        "UPTOBOX_TOKEN = \"OPTIONAL\" #@param {type:\"string\"}\n",
        "GO_FILE_PASS = \"OPTIONAL\" #@param {type:\"string\"}\n",
        "\n",
        "print(direct_link_generator(url))"
      ],
      "metadata": {
        "id": "lLQF6jtkNLl8",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "outputId": "56cf5610-b047-4750-ae62-0a52f5b6e0be"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/site-packages (2.22.0)\n",
            "Collecting lk21\n",
            "  Downloading lk21-1.6.0-py3-none-any.whl (44 kB)\n",
            "\u001b[K     || 44 kB 1.5 MB/s \n",
            "\u001b[?25hCollecting cfscrape\n",
            "  Downloading cfscrape-2.1.1-py3-none-any.whl (12 kB)\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Collecting cloudscraper\n",
            "  Downloading cloudscraper-1.2.71-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[K     || 99 kB 4.5 MB/s \n",
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'hashlib' candidate (version 20081119 at https://files.pythonhosted.org/packages/74/bb/9003d081345e9f0451884146e9ea2cff6e4cc4deac9ffd4a9ee98b318a49/hashlib-20081119.zip#sha256=419de2fd10ae71ed9c6adcb55903f116abd1d8acc8c814dfd5f839b4d5013e38 (from https://pypi.org/simple/hashlib/))\n",
            "Reason for being yanked: This was only for Python <= 2.4. LONG obsolete.\u001b[0m\n",
            "\u001b[?25hCollecting hashlib\n",
            "  Downloading hashlib-20081119.zip (42 kB)\n",
            "\u001b[K     || 42 kB 1.5 MB/s \n",
            "\u001b[31mERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'lk21'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-5fc86f474caf>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murlparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munquote\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjson\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mloads\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjsonloads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlk21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBypass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcfscrape\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_scraper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lk21'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}